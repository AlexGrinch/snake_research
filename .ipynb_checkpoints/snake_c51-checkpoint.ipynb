{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from PIL import Image\n",
    "from collections import deque, namedtuple\n",
    "from IPython.display import HTML\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments import Snake\n",
    "from visual_utils import AgentViz\n",
    "from methods import DistQNetwork, ReplayMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snake class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f84e58ade10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = Snake()\n",
    "img = s.reset()\n",
    "s.plot_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f84e589feb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, r, done = s.step(2)\n",
    "s.plot_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeAgent:\n",
    "    \n",
    "    def __init__(self, num_atoms=21, model_name=\"baseline_agent\"):\n",
    "        \n",
    "        \"\"\"Class for training and evaluating DQN agent on Atari games\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        game_id: str\n",
    "            game identifier in gym environment, e.g. \"Pong\"\n",
    "        num_actions: int\n",
    "            number of actions the agent can take\n",
    "        model_name: str\n",
    "            name of the model\n",
    "        \"\"\"\n",
    "        \n",
    "        ############################ Game environment ############################\n",
    "        \n",
    "        self.train_env = Snake()\n",
    "        self.num_actions = 4\n",
    "            \n",
    "        self.path = \"snake_models\" + \"/\" + model_name\n",
    "        if not os.path.exists(self.path):\n",
    "            os.makedirs(self.path)\n",
    "        \n",
    "        ############################# Agent & Target #############################\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self.agent_net = DistQNetwork(self.num_actions, num_atoms=num_atoms, scope=\"agent\")\n",
    "        self.target_net = DistQNetwork(self.num_actions, num_atoms=num_atoms, scope=\"target\")\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        all_vars = tf.trainable_variables()\n",
    "        num_vars = len(all_vars) // 2\n",
    "        self.agent_vars = all_vars[:num_vars]\n",
    "        self.target_vars = all_vars[num_vars:]\n",
    "        \n",
    "    def set_parameters(self, \n",
    "                       replay_memory_size=50000,\n",
    "                       replay_start_size=10000,\n",
    "                       init_eps=1,\n",
    "                       final_eps=0.1,\n",
    "                       annealing_steps=100000,\n",
    "                       discount_factor=0.99,\n",
    "                       max_episode_length=2000):\n",
    "        \n",
    "        # create experience replay and fill it with random policy samples\n",
    "        self.rep_buffer = ReplayMemory(replay_memory_size)\n",
    "        frame_count = 0\n",
    "        while (frame_count < replay_start_size):\n",
    "            s = self.train_env.reset()\n",
    "            for time_step in range(max_episode_length):\n",
    "                a = np.random.randint(self.num_actions)\n",
    "                s_, r, end = self.train_env.step(a)\n",
    "                self.rep_buffer.push(s, a, np.sign(r), s_, end)\n",
    "                s = s_\n",
    "                frame_count += 1\n",
    "                if end:\n",
    "                    break\n",
    "                        \n",
    "        self.eps = init_eps\n",
    "        self.final_eps = final_eps\n",
    "        self.eps_drop = (init_eps - final_eps) / annealing_steps\n",
    "        self.gamma = discount_factor\n",
    "        self.max_ep_length = max_episode_length\n",
    "        \n",
    "    def train(self,\n",
    "              gpu_id=0,\n",
    "              batch_size=32,\n",
    "              agent_update_freq=4,\n",
    "              target_update_freq=5000,\n",
    "              tau=1,\n",
    "              max_num_episodes=100000,\n",
    "              max_num_epochs=50000,\n",
    "              performance_print_freq=100,\n",
    "              save_freq=10000):\n",
    "        \n",
    "        target_ops = self.update_target_graph(tau)\n",
    "        os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(self.init)\n",
    "            \n",
    "            train_rewards = []\n",
    "            frame_count = 0\n",
    "            episode_count = 0\n",
    "            num_epochs = 0\n",
    "            \n",
    "            while num_epochs < max_num_epochs:\n",
    "                \n",
    "                train_ep_reward = 0\n",
    "                \n",
    "                # reset the environment / start new game\n",
    "                s = self.train_env.reset()\n",
    "                for time_step in range(self.max_ep_length):\n",
    "                    \n",
    "                    # choose action e-greedily\n",
    "                    if np.random.rand(1) < self.eps:\n",
    "                        a = np.random.randint(self.num_actions)\n",
    "                    else:\n",
    "                        a = self.agent_net.get_q_argmax(sess, [s])\n",
    "                        \n",
    "                    # make step in the environment    \n",
    "                    s_, r, end = self.train_env.step(a)\n",
    "                    \n",
    "                    # save transition into experience replay\n",
    "                    self.rep_buffer.push(s, a, np.sign(r), s_, end)\n",
    "                    \n",
    "                    # update current state and statistics\n",
    "                    s = s_\n",
    "                    frame_count += 1\n",
    "                    train_ep_reward += r\n",
    "                    \n",
    "                    # reduce epsilon according to schedule\n",
    "                    if self.eps > self.final_eps:\n",
    "                        self.eps -= self.eps_drop\n",
    "                    \n",
    "                    # update network weights\n",
    "                    if frame_count % agent_update_freq == 0:\n",
    "                        \n",
    "                        batch = self.rep_buffer.get_batch(batch_size)\n",
    "                        \n",
    "                        # estimate right hand side of the Bellman equation\n",
    "                        max_actions = self.agent_net.get_q_argmax(sess, batch.s_)\n",
    "                        target_m = self.target_net.cat_proj(sess, batch.r, batch.s_, \n",
    "                                                            max_actions, batch.end)\n",
    "                        \n",
    "                        # update agent network\n",
    "                        self.agent_net.update(sess, batch.s, batch.a, target_m)\n",
    "                        \n",
    "                        # update target network\n",
    "                        if tau == 1:\n",
    "                            if frame_count % target_update_freq == 0:\n",
    "                                self.update_target_weights(sess, target_ops)\n",
    "                        else: self.update_target_weights(sess, target_ops)\n",
    "                    \n",
    "                    # make checkpoints of network weights and save learning curve\n",
    "                    if frame_count % save_freq == 1:\n",
    "                        num_epochs += 1\n",
    "                        try:\n",
    "                            self.saver.save(sess, self.path+\"/model\", global_step=num_epochs)\n",
    "                            np.savez(self.path+\"/learning_curve.npz\", r=train_rewards)\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    # if game is over, reset the environment\n",
    "                    if end: \n",
    "                        break\n",
    "                         \n",
    "                episode_count += 1\n",
    "                train_rewards.append(train_ep_reward)\n",
    "                \n",
    "                # print performance once in a while\n",
    "                if episode_count % performance_print_freq == 0:\n",
    "                    avg_reward = np.mean(train_rewards[-performance_print_freq:])\n",
    "                    max_reward = np.max(train_rewards[-performance_print_freq:])\n",
    "                    print(\"Train info:\", frame_count, avg_reward, max_reward)  \n",
    "\n",
    "    def update_target_graph(self, tau):\n",
    "        op_holder = []\n",
    "        for agnt, trgt in zip(self.agent_vars, self.target_vars):\n",
    "            op = trgt.assign(agnt.value()*tau + (1 - tau)*trgt.value())\n",
    "            op_holder.append(op)\n",
    "        return op_holder\n",
    "\n",
    "    def update_target_weights(self, sess, op_holder):\n",
    "        for op in op_holder:\n",
    "            sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = SnakeAgent(model_name=\"baseline_dist3\", num_atoms=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.set_parameters(max_episode_length=20000, replay_memory_size=50000, replay_start_size=10000, final_eps=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train info: 770 -0.93 0\n",
      "Train info: 1445 -0.92 0\n",
      "Train info: 2346 -0.83 1\n",
      "Train info: 3193 -0.84 1\n"
     ]
    }
   ],
   "source": [
    "aa.train(gpu_id=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_atoms = 101\n",
    "tf.reset_default_graph()\n",
    "agent_net = DistQNetwork(4, num_atoms=num_atoms, scope=\"agent\")\n",
    "saver = tf.train.Saver()\n",
    "env = Snake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(3)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "viz = AgentViz(sess, 'snake_models/baseline_dist3/model-30', agent_net, max_frames=200, grid_size=(2, 5), figsize=(20, 8))\n",
    "anim = matplotlib.animation.FuncAnimation(viz.fig, viz, frames=viz.max_frames)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
