{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "597d6989-74e7-4ec7-8903-9aea215cbb49"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "21a06ab6-fd5a-4c39-b641-680a372defe6"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from PIL import Image\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "37e925e6-6039-4e40-93da-e956e072b26c"
    }
   },
   "outputs": [],
   "source": [
    "from environments import Snake\n",
    "from methods import QNetwork, ReplayMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0758ef30-1060-425c-95a9-3ff29fe4987a"
    }
   },
   "source": [
    "# Snake class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "469b20c6-6493-484b-9fbf-ecc47e81ae51"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5035512be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = Snake()\n",
    "img = s.reset()\n",
    "s.plot_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "35968c5e-cc3d-4ac5-93df-102b5a00ca5b"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f50334d20f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, r, done = s.step(2)\n",
    "s.plot_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d8b5cee0-93e4-44e5-b48c-3e9d072b7326"
    }
   },
   "source": [
    "# NN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "acdfe421-fbe4-45da-9cb1-015fcef7ad94"
    }
   },
   "outputs": [],
   "source": [
    "class SnakeAgent:\n",
    "    \n",
    "    def __init__(self, model_name=\"baseline_agent\"):\n",
    "        \n",
    "        \"\"\"Class for training and evaluating DQN agent on Atari games\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        game_id: str\n",
    "            game identifier in gym environment, e.g. \"Pong\"\n",
    "        num_actions: int\n",
    "            number of actions the agent can take\n",
    "        model_name: str\n",
    "            name of the model\n",
    "        \"\"\"\n",
    "        \n",
    "        ############################ Game environment ############################\n",
    "        \n",
    "        self.train_env = Snake(grid_size=(4, 4))\n",
    "        self.num_actions = 3\n",
    "            \n",
    "        self.path = \"snake_models\" + \"/\" + model_name\n",
    "        if not os.path.exists(self.path):\n",
    "            os.makedirs(self.path)\n",
    "        \n",
    "        ############################# Agent & Target #############################\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self.agent_net = QNetwork(self.num_actions, state_shape=[4, 4, 1], \n",
    "                                  convs=[[16, 2, 1], [32, 1, 1]], scope=\"agent\")\n",
    "        self.target_net = QNetwork(self.num_actions, state_shape=[4, 4, 1],\n",
    "                                   convs=[[16, 2, 1], [32, 1, 1]], scope=\"target\")\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        all_vars = tf.trainable_variables()\n",
    "        num_vars = len(all_vars) // 2\n",
    "        self.agent_vars = all_vars[:num_vars]\n",
    "        self.target_vars = all_vars[num_vars:]\n",
    "        \n",
    "    def set_parameters(self, \n",
    "                       replay_memory_size=50000,\n",
    "                       replay_start_size=10000,\n",
    "                       init_eps=1,\n",
    "                       final_eps=0.02,\n",
    "                       annealing_steps=100000,\n",
    "                       discount_factor=0.99,\n",
    "                       max_episode_length=2000):\n",
    "        \n",
    "        # create experience replay and fill it with random policy samples\n",
    "        self.rep_buffer = ReplayMemory(replay_memory_size)\n",
    "        frame_count = 0\n",
    "        while (frame_count < replay_start_size):\n",
    "            s = self.train_env.reset()\n",
    "            for time_step in range(max_episode_length):\n",
    "                a = np.random.randint(self.num_actions)\n",
    "                s_, r, end = self.train_env.step(a)\n",
    "                self.rep_buffer.push(s, a, np.sign(r), s_, end)\n",
    "                s = s_\n",
    "                frame_count += 1\n",
    "                if end:\n",
    "                    break\n",
    "                        \n",
    "        self.eps = init_eps\n",
    "        self.final_eps = final_eps\n",
    "        self.eps_drop = (init_eps - final_eps) / annealing_steps\n",
    "        self.gamma = discount_factor\n",
    "        self.max_ep_length = max_episode_length\n",
    "        \n",
    "    def train(self,\n",
    "              gpu_id=0,\n",
    "              batch_size=32,\n",
    "              agent_update_freq=4,\n",
    "              target_update_freq=5000,\n",
    "              tau=1,\n",
    "              max_num_episodes=100000,\n",
    "              max_num_epochs=50000,\n",
    "              performance_print_freq=500,\n",
    "              save_freq=10000):\n",
    "        \n",
    "        target_ops = self.update_target_graph(tau)\n",
    "        \n",
    "        os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        \n",
    "        \n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(self.init)\n",
    "            \n",
    "            train_rewards = []\n",
    "            frame_counts = []\n",
    "            frame_count = 0\n",
    "            episode_count = 0\n",
    "            num_epochs = 0\n",
    "            \n",
    "            while num_epochs < max_num_epochs:\n",
    "                \n",
    "                train_ep_reward = 0\n",
    "                \n",
    "                # reset the environment / start new game\n",
    "                s = self.train_env.reset()\n",
    "                for time_step in range(self.max_ep_length):\n",
    "                    \n",
    "                    # choose action e-greedily\n",
    "                    if np.random.rand(1) < self.eps:\n",
    "                        a = np.random.randint(self.num_actions)\n",
    "                    else:\n",
    "                        a = self.agent_net.get_q_argmax(sess, [s])\n",
    "                        \n",
    "                    # make step in the environment    \n",
    "                    s_, r, end = self.train_env.step(a)\n",
    "                    train_ep_reward += r\n",
    "                    \n",
    "                    # save transition into experience replay\n",
    "                    self.rep_buffer.push(s, a, r, s_, end)\n",
    "                    \n",
    "                    # update current state and statistics\n",
    "                    s = s_\n",
    "                    frame_count += 1\n",
    "                    \n",
    "                    # reduce epsilon according to schedule\n",
    "                    if self.eps > self.final_eps:\n",
    "                        self.eps -= self.eps_drop\n",
    "                    \n",
    "                    # update network weights\n",
    "                    if frame_count % agent_update_freq == 0:\n",
    "                        \n",
    "                        batch = self.rep_buffer.get_batch(batch_size)\n",
    "                        \n",
    "                        # estimate right hand side of the Bellman equation\n",
    "                        max_actions = self.agent_net.get_q_argmax(sess, batch.s_)\n",
    "                        q_values = self.target_net.get_q_values(sess, batch.s_)\n",
    "                        double_q = q_values[np.arange(batch_size), max_actions]\n",
    "                        targets = batch.r + (self.gamma * double_q * batch.end)\n",
    "                        \n",
    "                        # update agent network\n",
    "                        self.agent_net.update(sess, batch.s, batch.a, targets)\n",
    "                        \n",
    "                        # update target network\n",
    "                        if tau == 1:\n",
    "                            if frame_count % target_update_freq == 0:\n",
    "                                self.update_target_weights(sess, target_ops)\n",
    "                        else: self.update_target_weights(sess, target_ops)\n",
    "                    \n",
    "                    # make checkpoints of network weights and save learning curve\n",
    "                    if frame_count % save_freq == 1:\n",
    "                        num_epochs += 1\n",
    "                        try:\n",
    "                            self.saver.save(sess, self.path+\"/model\", global_step=num_epochs)\n",
    "                            np.savez(self.path+\"/learning_curve.npz\", r=train_rewards, f=frame_counts)\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    # if game is over, reset the environment\n",
    "                    if end: \n",
    "                        break\n",
    "                         \n",
    "                episode_count += 1\n",
    "                train_rewards.append(train_ep_reward)\n",
    "                frame_counts.append(frame_count)\n",
    "                \n",
    "                # print performance once in a while\n",
    "                if episode_count % performance_print_freq == 0:\n",
    "                    avg_reward = np.mean(train_rewards[-performance_print_freq:])\n",
    "                    print(\"Train info:\", frame_count, avg_reward, self.eps)  \n",
    "\n",
    "    def update_target_graph(self, tau):\n",
    "        op_holder = []\n",
    "        for agnt, trgt in zip(self.agent_vars, self.target_vars):\n",
    "            op = trgt.assign(agnt.value()*tau + (1 - tau)*trgt.value())\n",
    "            op_holder.append(op)\n",
    "        return op_holder\n",
    "\n",
    "    def update_target_weights(self, sess, op_holder):\n",
    "        for op in op_holder:\n",
    "            sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "feaa050e-c990-4d84-94b7-fe0961f7d5ee"
    }
   },
   "outputs": [],
   "source": [
    "aa = SnakeAgent(model_name=\"baseline_dqn3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbpresent": {
     "id": "b97770f2-5d19-48ca-86ca-7339a66481eb"
    }
   },
   "outputs": [],
   "source": [
    "aa.set_parameters(max_episode_length=400, replay_memory_size=50000, replay_start_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbpresent": {
     "id": "09a28cf1-440e-4ea4-b0ea-36c4e4536795"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train info: 1991 -0.526 0.9804882000001021\n",
      "Train info: 4253 -0.436 0.958320600000218\n",
      "Train info: 6576 -0.482 0.9355552000003371\n",
      "Train info: 9082 -0.464 0.9109964000004656\n",
      "Train info: 11431 -0.49 0.887976200000586\n",
      "Train info: 14089 -0.422 0.8619278000007222\n",
      "Train info: 16606 -0.434 0.8372612000008512\n",
      "Train info: 19253 -0.424 0.8113206000009869\n",
      "Train info: 21996 -0.41 0.7844392000011275\n",
      "Train info: 24924 -0.384 0.7557448000012776\n",
      "Train info: 27997 -0.33 0.7256294000014352\n",
      "Train info: 30877 -0.346 0.6974054000015828\n",
      "Train info: 34098 -0.264 0.6658396000017479\n",
      "Train info: 37197 -0.286 0.6354694000019068\n",
      "Train info: 40553 -0.208 0.6025806000020788\n",
      "Train info: 43901 -0.238 0.5697702000022504\n",
      "Train info: 47308 -0.214 0.5363816000024251\n",
      "Train info: 50818 -0.142 0.501983600002605\n",
      "Train info: 54349 -0.208 0.4673798000026012\n",
      "Train info: 57849 -0.15 0.43307980000258633\n",
      "Train info: 61260 -0.094 0.39965200000257184\n",
      "Train info: 64872 -0.088 0.3642544000025565\n",
      "Train info: 68470 -0.026 0.3289940000025412\n",
      "Train info: 72277 -0.04 0.291685400002525\n",
      "Train info: 75828 -0.064 0.25688560000250993\n",
      "Train info: 79591 0.024 0.22000820000249394\n",
      "Train info: 83295 0.034 0.1837090000024782\n",
      "Train info: 86888 0.054 0.14849760000246293\n",
      "Train info: 90527 0.08 0.11283540000244746\n",
      "Train info: 93910 0.066 0.07968200000243308\n",
      "Train info: 97578 0.17 0.043735600002430784\n",
      "Train info: 100939 0.05 0.019990200002433314\n",
      "Train info: 104325 0.14 0.019990200002433314\n",
      "Train info: 107965 0.174 0.019990200002433314\n",
      "Train info: 111452 0.162 0.019990200002433314\n",
      "Train info: 115309 0.286 0.019990200002433314\n",
      "Train info: 118997 0.268 0.019990200002433314\n",
      "Train info: 122793 0.258 0.019990200002433314\n",
      "Train info: 126559 0.29 0.019990200002433314\n",
      "Train info: 130390 0.28 0.019990200002433314\n",
      "Train info: 134136 0.288 0.019990200002433314\n",
      "Train info: 138096 0.362 0.019990200002433314\n",
      "Train info: 142309 0.39 0.019990200002433314\n",
      "Train info: 146432 0.44 0.019990200002433314\n",
      "Train info: 150666 0.49 0.019990200002433314\n",
      "Train info: 154856 0.514 0.019990200002433314\n",
      "Train info: 159169 0.514 0.019990200002433314\n",
      "Train info: 163607 0.606 0.019990200002433314\n",
      "Train info: 168006 0.616 0.019990200002433314\n",
      "Train info: 172592 0.672 0.019990200002433314\n",
      "Train info: 177227 0.772 0.019990200002433314\n",
      "Train info: 182251 0.876 0.019990200002433314\n",
      "Train info: 187092 0.852 0.019990200002433314\n",
      "Train info: 192026 0.962 0.019990200002433314\n",
      "Train info: 197189 0.966 0.019990200002433314\n",
      "Train info: 202574 1.082 0.019990200002433314\n",
      "Train info: 208065 1.098 0.019990200002433314\n",
      "Train info: 214014 1.252 0.019990200002433314\n",
      "Train info: 219916 1.318 0.019990200002433314\n",
      "Train info: 226274 1.492 0.019990200002433314\n",
      "Train info: 232795 1.606 0.019990200002433314\n",
      "Train info: 239161 1.612 0.019990200002433314\n",
      "Train info: 245856 1.682 0.019990200002433314\n",
      "Train info: 252898 1.95 0.019990200002433314\n",
      "Train info: 260164 1.966 0.019990200002433314\n",
      "Train info: 267552 2.072 0.019990200002433314\n",
      "Train info: 275540 2.396 0.019990200002433314\n",
      "Train info: 283340 2.294 0.019990200002433314\n",
      "Train info: 291337 2.466 0.019990200002433314\n",
      "Train info: 299113 2.38 0.019990200002433314\n",
      "Train info: 307256 2.57 0.019990200002433314\n",
      "Train info: 315632 2.634 0.019990200002433314\n",
      "Train info: 324458 2.88 0.019990200002433314\n",
      "Train info: 333096 2.694 0.019990200002433314\n",
      "Train info: 341825 2.8 0.019990200002433314\n",
      "Train info: 350984 3.104 0.019990200002433314\n",
      "Train info: 360731 3.284 0.019990200002433314\n",
      "Train info: 370632 3.378 0.019990200002433314\n",
      "Train info: 379637 3.118 0.019990200002433314\n",
      "Train info: 389363 3.444 0.019990200002433314\n",
      "Train info: 399413 3.482 0.019990200002433314\n",
      "Train info: 409618 3.532 0.019990200002433314\n",
      "Train info: 419949 3.71 0.019990200002433314\n",
      "Train info: 430310 3.482 0.019990200002433314\n",
      "Train info: 440886 3.738 0.019990200002433314\n",
      "Train info: 451660 3.778 0.019990200002433314\n",
      "Train info: 463187 4.14 0.019990200002433314\n",
      "Train info: 475269 4.288 0.019990200002433314\n",
      "Train info: 486553 4.044 0.019990200002433314\n",
      "Train info: 498509 4.39 0.019990200002433314\n",
      "Train info: 511532 4.728 0.019990200002433314\n",
      "Train info: 524065 4.52 0.019990200002433314\n",
      "Train info: 536972 4.796 0.019990200002433314\n",
      "Train info: 549965 4.748 0.019990200002433314\n",
      "Train info: 563593 5.094 0.019990200002433314\n",
      "Train info: 577505 5.154 0.019990200002433314\n",
      "Train info: 592177 5.394 0.019990200002433314\n",
      "Train info: 605893 5.226 0.019990200002433314\n",
      "Train info: 619856 5.262 0.019990200002433314\n",
      "Train info: 635043 5.762 0.019990200002433314\n",
      "Train info: 649924 5.61 0.019990200002433314\n",
      "Train info: 665011 5.716 0.019990200002433314\n",
      "Train info: 680209 5.928 0.019990200002433314\n",
      "Train info: 696088 6.256 0.019990200002433314\n",
      "Train info: 713032 6.656 0.019990200002433314\n",
      "Train info: 729836 6.736 0.019990200002433314\n",
      "Train info: 746583 6.736 0.019990200002433314\n",
      "Train info: 763327 6.932 0.019990200002433314\n",
      "Train info: 780439 7.312 0.019990200002433314\n",
      "Train info: 798451 7.738 0.019990200002433314\n",
      "Train info: 816825 8.18 0.019990200002433314\n",
      "Train info: 834271 7.978 0.019990200002433314\n",
      "Train info: 852275 8.268 0.019990200002433314\n",
      "Train info: 869412 8.03 0.019990200002433314\n",
      "Train info: 887654 9.124 0.019990200002433314\n",
      "Train info: 906063 9.112 0.019990200002433314\n",
      "Train info: 925418 9.684 0.019990200002433314\n",
      "Train info: 944465 9.586 0.019990200002433314\n",
      "Train info: 963342 9.846 0.019990200002433314\n",
      "Train info: 982433 10.114 0.019990200002433314\n",
      "Train info: 1001208 9.736 0.019990200002433314\n",
      "Train info: 1020825 10.19 0.019990200002433314\n",
      "Train info: 1039706 9.922 0.019990200002433314\n",
      "Train info: 1059075 10.27 0.019990200002433314\n",
      "Train info: 1078915 10.692 0.019990200002433314\n",
      "Train info: 1098631 10.622 0.019990200002433314\n",
      "Train info: 1118536 10.834 0.019990200002433314\n",
      "Train info: 1138451 10.61 0.019990200002433314\n",
      "Train info: 1157905 10.594 0.019990200002433314\n",
      "Train info: 1176981 10.326 0.019990200002433314\n",
      "Train info: 1197186 11.07 0.019990200002433314\n",
      "Train info: 1217019 10.824 0.019990200002433314\n",
      "Train info: 1237354 11.068 0.019990200002433314\n",
      "Train info: 1257345 10.93 0.019990200002433314\n",
      "Train info: 1277580 10.868 0.019990200002433314\n",
      "Train info: 1298407 11.34 0.019990200002433314\n",
      "Train info: 1319126 11.242 0.019990200002433314\n",
      "Train info: 1339680 11.424 0.019990200002433314\n",
      "Train info: 1361070 11.712 0.019990200002433314\n",
      "Train info: 1382296 11.612 0.019990200002433314\n",
      "Train info: 1402823 11.414 0.019990200002433314\n",
      "Train info: 1423315 11.296 0.019990200002433314\n",
      "Train info: 1444342 11.482 0.019990200002433314\n",
      "Train info: 1465409 11.612 0.019990200002433314\n",
      "Train info: 1486666 11.876 0.019990200002433314\n",
      "Train info: 1508366 11.874 0.019990200002433314\n",
      "Train info: 1529013 11.372 0.019990200002433314\n",
      "Train info: 1550138 11.738 0.019990200002433314\n",
      "Train info: 1570821 11.326 0.019990200002433314\n",
      "Train info: 1591487 11.526 0.019990200002433314\n",
      "Train info: 1612437 11.648 0.019990200002433314\n",
      "Train info: 1633163 11.532 0.019990200002433314\n",
      "Train info: 1653783 11.456 0.019990200002433314\n",
      "Train info: 1674976 11.754 0.019990200002433314\n",
      "Train info: 1695858 11.714 0.019990200002433314\n",
      "Train info: 1717225 11.954 0.019990200002433314\n",
      "Train info: 1738041 11.532 0.019990200002433314\n",
      "Train info: 1758755 11.604 0.019990200002433314\n",
      "Train info: 1779957 11.736 0.019990200002433314\n",
      "Train info: 1800688 11.588 0.019990200002433314\n",
      "Train info: 1822051 11.928 0.019990200002433314\n",
      "Train info: 1843078 11.538 0.019990200002433314\n",
      "Train info: 1864268 11.806 0.019990200002433314\n",
      "Train info: 1885662 11.854 0.019990200002433314\n",
      "Train info: 1906292 11.682 0.019990200002433314\n",
      "Train info: 1927412 11.804 0.019990200002433314\n",
      "Train info: 1948656 11.908 0.019990200002433314\n",
      "Train info: 1969236 11.346 0.019990200002433314\n",
      "Train info: 1990062 11.668 0.019990200002433314\n",
      "Train info: 2010722 11.508 0.019990200002433314\n",
      "Train info: 2031853 11.678 0.019990200002433314\n",
      "Train info: 2053164 11.974 0.019990200002433314\n",
      "Train info: 2074340 11.838 0.019990200002433314\n",
      "Train info: 2094979 11.57 0.019990200002433314\n",
      "Train info: 2115323 11.256 0.019990200002433314\n",
      "Train info: 2136256 11.572 0.019990200002433314\n",
      "Train info: 2156858 11.43 0.019990200002433314\n",
      "Train info: 2177209 11.38 0.019990200002433314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train info: 2198101 11.798 0.019990200002433314\n",
      "Train info: 2219053 11.634 0.019990200002433314\n",
      "Train info: 2240111 11.868 0.019990200002433314\n",
      "Train info: 2260986 11.754 0.019990200002433314\n",
      "Train info: 2281922 11.86 0.019990200002433314\n",
      "Train info: 2303241 11.946 0.019990200002433314\n",
      "Train info: 2324222 11.844 0.019990200002433314\n",
      "Train info: 2345737 12.11 0.019990200002433314\n",
      "Train info: 2367019 11.908 0.019990200002433314\n",
      "Train info: 2387465 11.504 0.019990200002433314\n",
      "Train info: 2408958 11.972 0.019990200002433314\n",
      "Train info: 2430282 11.846 0.019990200002433314\n",
      "Train info: 2451433 11.752 0.019990200002433314\n",
      "Train info: 2472963 12.188 0.019990200002433314\n",
      "Train info: 2494224 11.858 0.019990200002433314\n",
      "Train info: 2515187 12.036 0.019990200002433314\n",
      "Train info: 2536276 11.892 0.019990200002433314\n",
      "Train info: 2556749 11.592 0.019990200002433314\n",
      "Train info: 2577719 11.606 0.019990200002433314\n",
      "Train info: 2598716 11.77 0.019990200002433314\n",
      "Train info: 2619668 11.73 0.019990200002433314\n",
      "Train info: 2640910 12.104 0.019990200002433314\n",
      "Train info: 2661049 11.24 0.019990200002433314\n",
      "Train info: 2681668 11.376 0.019990200002433314\n",
      "Train info: 2702451 11.77 0.019990200002433314\n",
      "Train info: 2723556 11.936 0.019990200002433314\n",
      "Train info: 2744536 11.698 0.019990200002433314\n",
      "Train info: 2764886 11.388 0.019990200002433314\n",
      "Train info: 2785670 11.592 0.019990200002433314\n",
      "Train info: 2806344 11.37 0.019990200002433314\n",
      "Train info: 2826987 11.662 0.019990200002433314\n",
      "Train info: 2847763 11.688 0.019990200002433314\n",
      "Train info: 2867976 11.482 0.019990200002433314\n",
      "Train info: 2889164 11.944 0.019990200002433314\n",
      "Train info: 2909555 11.452 0.019990200002433314\n",
      "Train info: 2930364 11.52 0.019990200002433314\n",
      "Train info: 2952064 12.216 0.019990200002433314\n",
      "Train info: 2972938 11.53 0.019990200002433314\n",
      "Train info: 2993909 11.692 0.019990200002433314\n",
      "Train info: 3014315 11.096 0.019990200002433314\n",
      "Train info: 3034880 11.634 0.019990200002433314\n",
      "Train info: 3055864 11.692 0.019990200002433314\n",
      "Train info: 3075964 11.302 0.019990200002433314\n",
      "Train info: 3096763 11.656 0.019990200002433314\n",
      "Train info: 3116692 11.022 0.019990200002433314\n",
      "Train info: 3137480 11.52 0.019990200002433314\n",
      "Train info: 3158217 11.374 0.019990200002433314\n",
      "Train info: 3179433 11.816 0.019990200002433314\n",
      "Train info: 3200379 11.898 0.019990200002433314\n",
      "Train info: 3221446 11.68 0.019990200002433314\n",
      "Train info: 3241899 11.406 0.019990200002433314\n",
      "Train info: 3262684 11.508 0.019990200002433314\n",
      "Train info: 3283184 11.456 0.019990200002433314\n",
      "Train info: 3303861 11.576 0.019990200002433314\n",
      "Train info: 3324901 11.656 0.019990200002433314\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-35eaaaae7f11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-a35a115b18d6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, gpu_id, batch_size, agent_update_freq, target_update_freq, tau, max_num_episodes, max_num_epochs, performance_print_freq, save_freq)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                         \u001b[0;31m# update agent network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                         \u001b[0;31m# update target network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/snake_research/methods.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, sess, states, actions, targets)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "aa.train(gpu_id=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ef0de02b-c5a1-4823-9880-bff23962246e"
    }
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "3f689dd3-93bc-4558-a105-8317e3c53010"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "agent_net = QNetwork(4, scope=\"agent\")\n",
    "saver = tf.train.Saver()\n",
    "env = Snake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "67311291-d8d1-4255-a4bb-42c367d9d81d"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(3)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"snake_models/baseline_dqn2/model-2618\")\n",
    "    s = env.reset()\n",
    "    for i in range(1000):\n",
    "        a = agent_net.get_q_argmax(sess, [s])[0]\n",
    "        s, r, done = env.step(a)\n",
    "        \n",
    "        \n",
    "        env.plot_state()\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        #time.sleep(0.01)\n",
    "        \n",
    "        if done: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curve = np.load('snake_models/baseline_dqn2/learning_curve.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vals = curve['r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_average(r, eta):\n",
    "    ret = np.cumsum(r, dtype=np.float)\n",
    "    ret[eta:] = ret[eta:] - ret[:-eta] \n",
    "    y = ret[eta - 1:] / eta\n",
    "    return y\n",
    "\n",
    "def plot_performance(path, eta=10):\n",
    "    r_train = np.load(path + \"/learning_curve.npz\")[\"r\"]\n",
    "    y = plot_average(r_train, eta)\n",
    "    plt.plot(y)\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_performance('snake_models/baseline_dqn2', eta=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
