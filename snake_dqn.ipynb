{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "597d6989-74e7-4ec7-8903-9aea215cbb49"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "21a06ab6-fd5a-4c39-b641-680a372defe6"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from PIL import Image\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "37e925e6-6039-4e40-93da-e956e072b26c"
    }
   },
   "outputs": [],
   "source": [
    "from environments import Snake\n",
    "from methods import QNetwork, ReplayMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0758ef30-1060-425c-95a9-3ff29fe4987a"
    }
   },
   "source": [
    "# Snake class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "469b20c6-6493-484b-9fbf-ecc47e81ae51"
    }
   },
   "outputs": [],
   "source": [
    "s = Snake()\n",
    "img = s.reset()\n",
    "s.plot_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "35968c5e-cc3d-4ac5-93df-102b5a00ca5b"
    }
   },
   "outputs": [],
   "source": [
    "img, r, done = s.step(2)\n",
    "s.plot_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d8b5cee0-93e4-44e5-b48c-3e9d072b7326"
    }
   },
   "source": [
    "# NN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "acdfe421-fbe4-45da-9cb1-015fcef7ad94"
    }
   },
   "outputs": [],
   "source": [
    "class SnakeAgent:\n",
    "    \n",
    "    def __init__(self, model_name=\"baseline_agent\"):\n",
    "        \n",
    "        \"\"\"Class for training and evaluating DQN agent on Atari games\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        game_id: str\n",
    "            game identifier in gym environment, e.g. \"Pong\"\n",
    "        num_actions: int\n",
    "            number of actions the agent can take\n",
    "        model_name: str\n",
    "            name of the model\n",
    "        \"\"\"\n",
    "        \n",
    "        ############################ Game environment ############################\n",
    "        \n",
    "        self.train_env = Snake()\n",
    "        self.num_actions = 4\n",
    "            \n",
    "        self.path = \"snake_models\" + \"/\" + model_name\n",
    "        if not os.path.exists(self.path):\n",
    "            os.makedirs(self.path)\n",
    "        \n",
    "        ############################# Agent & Target #############################\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self.agent_net = QNetwork(self.num_actions, scope=\"agent\")\n",
    "        self.target_net = QNetwork(self.num_actions, scope=\"target\")\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        all_vars = tf.trainable_variables()\n",
    "        num_vars = len(all_vars) // 2\n",
    "        self.agent_vars = all_vars[:num_vars]\n",
    "        self.target_vars = all_vars[num_vars:]\n",
    "        \n",
    "    def set_parameters(self, \n",
    "                       replay_memory_size=50000,\n",
    "                       replay_start_size=10000,\n",
    "                       init_eps=1,\n",
    "                       final_eps=0.1,\n",
    "                       annealing_steps=100000,\n",
    "                       discount_factor=0.99,\n",
    "                       max_episode_length=2000):\n",
    "        \n",
    "        # create experience replay and fill it with random policy samples\n",
    "        self.rep_buffer = ReplayMemory(replay_memory_size)\n",
    "        frame_count = 0\n",
    "        while (frame_count < replay_start_size):\n",
    "            s = self.train_env.reset()\n",
    "            for time_step in range(max_episode_length):\n",
    "                a = np.random.randint(self.num_actions)\n",
    "                s_, r, end = self.train_env.step(a)\n",
    "                self.rep_buffer.push(s, a, np.sign(r), s_, end)\n",
    "                s = s_\n",
    "                frame_count += 1\n",
    "                if end:\n",
    "                    break\n",
    "                        \n",
    "        self.eps = init_eps\n",
    "        self.final_eps = final_eps\n",
    "        self.eps_drop = (init_eps - final_eps) / annealing_steps\n",
    "        self.gamma = discount_factor\n",
    "        self.max_ep_length = max_episode_length\n",
    "        \n",
    "    def train(self,\n",
    "              gpu_id=0,\n",
    "              batch_size=32,\n",
    "              agent_update_freq=4,\n",
    "              target_update_freq=5000,\n",
    "              tau=1,\n",
    "              max_num_episodes=100000,\n",
    "              max_num_epochs=50000,\n",
    "              performance_print_freq=100,\n",
    "              save_freq=10000):\n",
    "        \n",
    "        target_ops = self.update_target_graph(tau)\n",
    "        \n",
    "        os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        \n",
    "        \n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(self.init)\n",
    "            \n",
    "            train_rewards = []\n",
    "            frame_count = 0\n",
    "            episode_count = 0\n",
    "            num_epochs = 0\n",
    "            \n",
    "            while num_epochs < max_num_epochs:\n",
    "                \n",
    "                train_ep_reward = 0\n",
    "                \n",
    "                # reset the environment / start new game\n",
    "                s = self.train_env.reset()\n",
    "                for time_step in range(self.max_ep_length):\n",
    "                    \n",
    "                    # choose action e-greedily\n",
    "                    if np.random.rand(1) < self.eps:\n",
    "                        a = np.random.randint(self.num_actions)\n",
    "                    else:\n",
    "                        a = self.agent_net.get_q_argmax(sess, [s])\n",
    "                        \n",
    "                    # make step in the environment    \n",
    "                    s_, r, end = self.train_env.step(a)\n",
    "                    \n",
    "                    # save transition into experience replay\n",
    "                    self.rep_buffer.push(s, a, np.sign(r), s_, end)\n",
    "                    \n",
    "                    # update current state and statistics\n",
    "                    s = s_\n",
    "                    frame_count += 1\n",
    "                    train_ep_reward += r\n",
    "                    \n",
    "                    # reduce epsilon according to schedule\n",
    "                    if self.eps > self.final_eps:\n",
    "                        self.eps -= self.eps_drop\n",
    "                    \n",
    "                    # update network weights\n",
    "                    if frame_count % agent_update_freq == 0:\n",
    "                        \n",
    "                        batch = self.rep_buffer.get_batch(batch_size)\n",
    "                        \n",
    "                        # estimate right hand side of the Bellman equation\n",
    "                        max_actions = self.agent_net.get_q_argmax(sess, batch.s_)\n",
    "                        q_values = self.target_net.get_q_values(sess, batch.s_)\n",
    "                        double_q = q_values[np.arange(batch_size), max_actions]\n",
    "                        targets = batch.r + (self.gamma * double_q * batch.end)\n",
    "                        \n",
    "                        # update agent network\n",
    "                        self.agent_net.update(sess, batch.s, batch.a, targets)\n",
    "                        \n",
    "                        # update target network\n",
    "                        if tau == 1:\n",
    "                            if frame_count % target_update_freq == 0:\n",
    "                                self.update_target_weights(sess, target_ops)\n",
    "                        else: self.update_target_weights(sess, target_ops)\n",
    "                    \n",
    "                    # make checkpoints of network weights and save learning curve\n",
    "                    if frame_count % save_freq == 1:\n",
    "                        num_epochs += 1\n",
    "                        try:\n",
    "                            self.saver.save(sess, self.path+\"/model\", global_step=num_epochs)\n",
    "                            np.savez(self.path+\"/learning_curve.npz\", r=train_rewards)\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    # if game is over, reset the environment\n",
    "                    if end: \n",
    "                        break\n",
    "                         \n",
    "                episode_count += 1\n",
    "                train_rewards.append(train_ep_reward)\n",
    "                \n",
    "                # print performance once in a while\n",
    "                if episode_count % performance_print_freq == 0:\n",
    "                    avg_reward = np.mean(train_rewards[-performance_print_freq:])\n",
    "                    print(\"Train info:\", frame_count, avg_reward, self.eps)  \n",
    "\n",
    "    def update_target_graph(self, tau):\n",
    "        op_holder = []\n",
    "        for agnt, trgt in zip(self.agent_vars, self.target_vars):\n",
    "            op = trgt.assign(agnt.value()*tau + (1 - tau)*trgt.value())\n",
    "            op_holder.append(op)\n",
    "        return op_holder\n",
    "\n",
    "    def update_target_weights(self, sess, op_holder):\n",
    "        for op in op_holder:\n",
    "            sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "feaa050e-c990-4d84-94b7-fe0961f7d5ee"
    }
   },
   "outputs": [],
   "source": [
    "aa = SnakeAgent(model_name=\"baseline_dqn2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "b97770f2-5d19-48ca-86ca-7339a66481eb"
    }
   },
   "outputs": [],
   "source": [
    "aa.set_parameters(max_episode_length=50000, replay_memory_size=50000, replay_start_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "09a28cf1-440e-4ea4-b0ea-36c4e4536795"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aa.train(gpu_id=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ef0de02b-c5a1-4823-9880-bff23962246e"
    }
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "3f689dd3-93bc-4558-a105-8317e3c53010"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "agent_net = QNetwork(4, scope=\"agent\")\n",
    "saver = tf.train.Saver()\n",
    "env = Snake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "67311291-d8d1-4255-a4bb-42c367d9d81d"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(3)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"snake_models/baseline_dqn2/model-2618\")\n",
    "    s = env.reset()\n",
    "    for i in range(1000):\n",
    "        a = agent_net.get_q_argmax(sess, [s])[0]\n",
    "        s, r, done = env.step(a)\n",
    "        \n",
    "        \n",
    "        env.plot_state()\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        #time.sleep(0.01)\n",
    "        \n",
    "        if done: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curve = np.load('snake_models/baseline_dqn2/learning_curve.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vals = curve['r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_average(r, eta):\n",
    "    ret = np.cumsum(r, dtype=np.float)\n",
    "    ret[eta:] = ret[eta:] - ret[:-eta] \n",
    "    y = ret[eta - 1:] / eta\n",
    "    return y\n",
    "\n",
    "def plot_performance(path, eta=10):\n",
    "    r_train = np.load(path + \"/learning_curve.npz\")[\"r\"]\n",
    "    y = plot_average(r_train, eta)\n",
    "    plt.plot(y)\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_performance('snake_models/baseline_dqn2', eta=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tf)",
   "language": "python",
   "name": "conda_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
