{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from PIL import Image\n",
    "from collections import deque, namedtuple\n",
    "from IPython.display import HTML\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from environments import Snake\n",
    "from visual_utils import AgentViz\n",
    "from methods import DistQNetwork, QNetwork, ReplayMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snake environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAACfRJREFUeJzt3eGrZHUdx/H3p1XbNFMoC3UlfRBCBGUsmhhCSmEpVtADBYMk2EeKUhDWs/4BqwcRyKYFmVKWEGGZlGFBmbvrVrqrsS2Gu2lrRGhGbua3B3c2Ntm45+6cc2fut/cLLt6Ze7jzHZa358y5M+eXqkJST69Z9ACSpmPgUmMGLjVm4FJjBi41ZuBSYwYuNWbgUmMGLjV2whS/9KS8tjZzyhS/WhLwD17kcL2U1babJPDNnMJFuXyKXy0JeLh+PGg7D9GlxgxcaszApcYMXGrMwKXGDFxqzMClxgxcamxQ4EmuSPJkkn1Jbpl6KEnjWDXwJJuALwMfBN4OXJvk7VMPJml+Q/bgFwL7qmp/VR0G7gY+PO1YksYwJPCzgaePun1gdp+kJTfah02SbAO2AWzm5LF+raQ5DNmDHwTOOer2ltl9/6WqbquqrVW19UReO9Z8kuYwJPBHgLclOS/JScA1wPemHUvSGFY9RK+ql5PcANwPbAJur6rHJ59M0twGvQavqvuA+yaeRdLIfCeb1JiBS40ZuNSYgUuNGbjUmIFLjRm41JiBS40ZuNSYgUuNGbjUmIFLjRm41JiBS40ZuNSYgUuNGbjUmIFLjQ1Z2eT2JIeSPLYeA0kaz5A9+NeAKyaeQ9IEVg28qh4C/rIOs0gama/BpcZcukhqbLQ9uEsXScvHQ3SpsSF/JrsL+AVwfpIDST45/ViSxjBkbbJr12MQSePzEF1qzMClxgxcaszApcYMXGrMwKXGDFxqzMClxgxcaszApcYMXGrMwKXGDFxqzMClxgxcaszApcYMXGrMwKXGDFxqbMhFF89J8mCSPUkeT3LTegwmaX5DFj54Gfh0Ve1KciqwM8kDVbVn4tkkzWnI2mTPVNWu2fcvAHuBs6ceTNL81rR0UZJzgQuAh4/xM5cukpbM4JNsSV4PfAe4uaqef/XPXbpIWj6DAk9yIitx31lV3512JEljGXIWPcBXgb1Vdev0I0kay5A9+CXAx4HLkuyefX1o4rkkjWDI2mQ/B7IOs0game9kkxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqbEhF13cnORXSX49W7ro8+sxmKT5DVn44CXgsqr62+zyyT9P8oOq+uXEs0ma05CLLhbwt9nNE2dfNeVQksYxdOGDTUl2A4eAB6rqmEsXJdmRZMc/eWnsOSUdh0GBV9W/qupdwBbgwiTvOMY2Ll0kLZk1nUWvqr8CDwJXTDOOpDENOYt+RpLTZ9+/Dng/8MTUg0ma35Cz6GcCX0+yiZX/IXyrqr4/7ViSxjDkLPpvWFkTXNIG4zvZpMYMXGrMwKXGDFxqzMClxgxcaszApcYMXGrMwKXGDFxqzMClxgxcaszApcYMXGrMwKXGDFxqzMClxgxcamxw4LNroz+axOuxSRvEWvbgNwF7pxpE0viGrmyyBbgS2D7tOJLGNHQP/kXgM8ArE84iaWRDFj64CjhUVTtX2c61yaQlM2QPfglwdZKngLuBy5J849UbuTaZtHxWDbyqPltVW6rqXOAa4CdVdd3kk0mam38HlxobsjbZf1TVT4GfTjKJpNG5B5caM3CpMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKmxQZdsml1R9QXgX8DLVbV1yqEkjWMt12R7X1X9ebJJJI3OQ3SpsaGBF/CjJDuTbJtyIEnjGXqI/t6qOpjkzcADSZ6oqoeO3mAW/jaAzZw88piSjsegPXhVHZz99xBwL3DhMbZx6SJpyQxZfPCUJKce+R74APDY1INJmt+QQ/S3APcmObL9N6vqh5NOJWkUqwZeVfuBd67DLJJG5p/JpMYMXGrMwKXGDFxqzMClxgxcaszApcYMXGpsLZ8Hl0bx949etG6PdfK9D6/bYy0j9+BSYwYuNWbgUmMGLjVm4FJjBi41ZuBSYwYuNWbgUmODAk9yepJ7kjyRZG+Si6ceTNL8hr5V9UvAD6vqY0lOAi98Lm0Eqwae5DTgUuATAFV1GDg87ViSxjDkEP084DngjiSPJtk+uz66pCU3JPATgHcDX6mqC4AXgVtevVGSbUl2JNnxT14aeUxJx2NI4AeAA1V15HN397AS/H9x6SJp+awaeFU9Czyd5PzZXZcDeyadStIohp5FvxG4c3YGfT9w/XQjSRrLoMCrajewdeJZJI3Md7JJjRm41JiBS40ZuNSYgUuNGbjUmIFLjRm41JiBS41t+LXJzvrlqev2WH98zwvr9ljrbd8X3rNuj3XWQ7Vuj/X/zj241JiBS40ZuNSYgUuNGbjUmIFLjRm41JiBS40ZuNTYqoEnOT/J7qO+nk9y83oMJ2k+q75VtaqeBN4FkGQTcBC4d+K5JI1grYfolwO/r6o/TDGMpHGt9cMm1wB3HesHSbYB2wA2u/iotBQG78Fnix5cDXz7WD936SJp+azlEP2DwK6q+tNUw0ga11oCv5b/cXguaTkNCny2Hvj7ge9OO46kMQ1dm+xF4I0TzyJpZL6TTWrMwKXGDFxqzMClxgxcaszApcYMXGrMwKXGUjX+MjJJngPW+pHSNwF/Hn2Y5dD1ufm8FuetVXXGahtNEvjxSLKjqrYueo4pdH1uPq/l5yG61JiBS40tU+C3LXqACXV9bj6vJbc0r8EljW+Z9uCSRrYUgSe5IsmTSfYluWXR84whyTlJHkyyJ8njSW5a9ExjSrIpyaNJvr/oWcaU5PQk9yR5IsneJBcveqZ5LPwQfXat9d+xcsWYA8AjwLVVtWehg80pyZnAmVW1K8mpwE7gIxv9eR2R5FPAVuANVXXVoucZS5KvAz+rqu2zC42eXFV/XfRcx2sZ9uAXAvuqan9VHQbuBj684JnmVlXPVNWu2fcvAHuBsxc71TiSbAGuBLYvepYxJTkNuBT4KkBVHd7IccNyBH428PRRtw/QJIQjkpwLXAA8vNhJRvNF4DPAK4seZGTnAc8Bd8xefmyfXY9ww1qGwFtL8nrgO8DNVfX8oueZV5KrgENVtXPRs0zgBODdwFeq6gLgRWBDnxNahsAPAuccdXvL7L4NL8mJrMR9Z1V1uSLtJcDVSZ5i5eXUZUm+sdiRRnMAOFBVR4607mEl+A1rGQJ/BHhbkvNmJzWuAb634JnmliSsvJbbW1W3LnqesVTVZ6tqS1Wdy8q/1U+q6roFjzWKqnoWeDrJ+bO7Lgc29EnRta5NNrqqejnJDcD9wCbg9qp6fMFjjeES4OPAb5Psnt33uaq6b4EzaXU3AnfOdjb7gesXPM9cFv5nMknTWYZDdEkTMXCpMQOXGjNwqTEDlxozcKkxA5caM3CpsX8DC3thGzDou6oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fed9deae320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = Snake()\n",
    "img = s.reset()\n",
    "s.plot_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fef5bfc9208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, r, done = s.step(2)\n",
    "s.plot_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SnakeAgent:\n",
    "    \n",
    "    def __init__(self, num_atoms=21, model_name=\"baseline_agent\"):\n",
    "        \n",
    "        \"\"\"Class for training and evaluating DQN agent on Atari games\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        game_id: str\n",
    "            game identifier in gym environment, e.g. \"Pong\"\n",
    "        num_actions: int\n",
    "            number of actions the agent can take\n",
    "        model_name: str\n",
    "            name of the model\n",
    "        \"\"\"\n",
    "        \n",
    "        ############################ Game environment ############################\n",
    "        \n",
    "        self.train_env = Snake()\n",
    "        self.num_actions = 4\n",
    "            \n",
    "        self.path = \"snake_models\" + \"/\" + model_name\n",
    "        if not os.path.exists(self.path):\n",
    "            os.makedirs(self.path)\n",
    "        \n",
    "        ############################# Agent & Target #############################\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self.agent_net = DistQNetwork(self.num_actions, num_atoms=num_atoms, scope=\"agent\")\n",
    "        self.target_net = DistQNetwork(self.num_actions, num_atoms=num_atoms, scope=\"target\")\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        all_vars = tf.trainable_variables()\n",
    "        num_vars = len(all_vars) // 2\n",
    "        self.agent_vars = all_vars[:num_vars]\n",
    "        self.target_vars = all_vars[num_vars:]\n",
    "        \n",
    "    def set_parameters(self, \n",
    "                       replay_memory_size=50000,\n",
    "                       replay_start_size=10000,\n",
    "                       init_eps=1,\n",
    "                       final_eps=0.1,\n",
    "                       annealing_steps=100000,\n",
    "                       discount_factor=0.99,\n",
    "                       max_episode_length=2000):\n",
    "        \n",
    "        # create experience replay and fill it with random policy samples\n",
    "        self.rep_buffer = ReplayMemory(replay_memory_size)\n",
    "        frame_count = 0\n",
    "        while (frame_count < replay_start_size):\n",
    "            s = self.train_env.reset()\n",
    "            for time_step in range(max_episode_length):\n",
    "                a = np.random.randint(self.num_actions)\n",
    "                s_, r, end = self.train_env.step(a)\n",
    "                self.rep_buffer.push(s, a, np.sign(r), s_, end)\n",
    "                s = s_\n",
    "                frame_count += 1\n",
    "                if end:\n",
    "                    break\n",
    "                        \n",
    "        self.eps = init_eps\n",
    "        self.final_eps = final_eps\n",
    "        self.eps_drop = (init_eps - final_eps) / annealing_steps\n",
    "        self.gamma = discount_factor\n",
    "        self.max_ep_length = max_episode_length\n",
    "        \n",
    "    def train(self,\n",
    "              gpu_id=0,\n",
    "              batch_size=32,\n",
    "              agent_update_freq=4,\n",
    "              target_update_freq=5000,\n",
    "              tau=1,\n",
    "              max_num_episodes=100000,\n",
    "              max_num_epochs=50000,\n",
    "              performance_print_freq=100,\n",
    "              save_freq=10000,\n",
    "              from_epoch=0):\n",
    "        \n",
    "        target_ops = self.update_target_graph(tau)\n",
    "        os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        with tf.Session(config=config) as sess:\n",
    "            \n",
    "            if from_epoch == 0:\n",
    "                train_rewards = []\n",
    "                num_epochs = 0\n",
    "                sess.run(self.init)\n",
    "            else:\n",
    "                self.saver.restore(sess, self.path+\"/model-\"+str(from_epoch))\n",
    "                train_rewards = list(np.load(self.path+\"/learning_curve.npz\")[\"r\"])\n",
    "                num_epochs = from_epoch\n",
    "            \n",
    "            frame_count = 0\n",
    "            episode_count = 0\n",
    "            \n",
    "            while num_epochs < max_num_epochs:\n",
    "                \n",
    "                train_ep_reward = 0\n",
    "                \n",
    "                # reset the environment / start new game\n",
    "                s = self.train_env.reset()\n",
    "                for time_step in range(self.max_ep_length):\n",
    "                    \n",
    "                    # choose action e-greedily\n",
    "                    if np.random.rand(1) < self.eps:\n",
    "                        a = np.random.randint(self.num_actions)\n",
    "                    else:\n",
    "                        a = self.agent_net.get_q_argmax(sess, [s])\n",
    "                        \n",
    "                    # make step in the environment    \n",
    "                    s_, r, end = self.train_env.step(a)\n",
    "                    \n",
    "                    # save transition into experience replay\n",
    "                    self.rep_buffer.push(s, a, np.sign(r), s_, end)\n",
    "                    \n",
    "                    # update current state and statistics\n",
    "                    s = s_\n",
    "                    frame_count += 1\n",
    "                    train_ep_reward += r\n",
    "                    \n",
    "                    # reduce epsilon according to schedule\n",
    "                    if self.eps > self.final_eps:\n",
    "                        self.eps -= self.eps_drop\n",
    "                    \n",
    "                    # update network weights\n",
    "                    if frame_count % agent_update_freq == 0:\n",
    "                        \n",
    "                        batch = self.rep_buffer.get_batch(batch_size)\n",
    "                        \n",
    "                        # estimate right hand side of the Bellman equation\n",
    "                        max_actions = self.agent_net.get_q_argmax(sess, batch.s_)\n",
    "                        target_m = self.target_net.cat_proj(sess, batch.r, batch.s_, \n",
    "                                                            max_actions, batch.end)\n",
    "                        \n",
    "                        # update agent network\n",
    "                        self.agent_net.update(sess, batch.s, batch.a, target_m)\n",
    "                        \n",
    "                        # update target network\n",
    "                        if tau == 1:\n",
    "                            if frame_count % target_update_freq == 0:\n",
    "                                self.update_target_weights(sess, target_ops)\n",
    "                        else: self.update_target_weights(sess, target_ops)\n",
    "                    \n",
    "                    # make checkpoints of network weights and save learning curve\n",
    "                    if frame_count % save_freq == 1:\n",
    "                        num_epochs += 1\n",
    "                        try:\n",
    "                            self.saver.save(sess, self.path+\"/model\", global_step=num_epochs)\n",
    "                            np.savez(self.path+\"/learning_curve.npz\", r=train_rewards)\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    # if game is over, reset the environment\n",
    "                    if end: \n",
    "                        break\n",
    "                         \n",
    "                episode_count += 1\n",
    "                train_rewards.append(train_ep_reward)\n",
    "                \n",
    "                # print performance once in a while\n",
    "                if episode_count % performance_print_freq == 0:\n",
    "                    avg_reward = np.mean(train_rewards[-performance_print_freq:])\n",
    "                    max_reward = np.max(train_rewards[-performance_print_freq:])\n",
    "                    print(\"Train info:\", frame_count, avg_reward, max_reward)  \n",
    "\n",
    "    def update_target_graph(self, tau):\n",
    "        op_holder = []\n",
    "        for agnt, trgt in zip(self.agent_vars, self.target_vars):\n",
    "            op = trgt.assign(agnt.value()*tau + (1 - tau)*trgt.value())\n",
    "            op_holder.append(op)\n",
    "        return op_holder\n",
    "\n",
    "    def update_target_weights(self, sess, op_holder):\n",
    "        for op in op_holder:\n",
    "            sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa = SnakeAgent(model_name=\"dist51_2\", num_atoms=51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa.set_parameters(max_episode_length=100, replay_memory_size=50000, replay_start_size=50, init_eps=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from snake_models/dist51_2/model-3908\n",
      "Train info: 5993 5.68 14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-94643c80e245>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3908\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-a2690f20528e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, gpu_id, batch_size, agent_update_freq, target_update_freq, tau, max_num_episodes, max_num_epochs, performance_print_freq, save_freq, from_epoch)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                         \u001b[0;31m# update agent network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                         \u001b[0;31m# update target network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/snake_research/methods.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, sess, states, actions, targets)\u001b[0m\n\u001b[1;32m    240\u001b[0m                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_actions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                      self.targets:targets}\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcat_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "aa.train(gpu_id=2, from_epoch=3908)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
