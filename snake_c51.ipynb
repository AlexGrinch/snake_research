{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from PIL import Image\n",
    "from collections import deque, namedtuple\n",
    "from IPython.display import HTML\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments import Snake\n",
    "from visual_utils import AgentViz\n",
    "from methods import DistQNetwork, ReplayMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snake class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb56b312dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = Snake()\n",
    "img = s.reset()\n",
    "s.plot_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f84e589feb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, r, done = s.step(2)\n",
    "s.plot_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeAgent:\n",
    "    \n",
    "    def __init__(self, num_atoms=21, model_name=\"baseline_agent\"):\n",
    "        \n",
    "        \"\"\"Class for training and evaluating DQN agent on Atari games\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        game_id: str\n",
    "            game identifier in gym environment, e.g. \"Pong\"\n",
    "        num_actions: int\n",
    "            number of actions the agent can take\n",
    "        model_name: str\n",
    "            name of the model\n",
    "        \"\"\"\n",
    "        \n",
    "        ############################ Game environment ############################\n",
    "        \n",
    "        self.train_env = Snake()\n",
    "        self.num_actions = 4\n",
    "            \n",
    "        self.path = \"snake_models\" + \"/\" + model_name\n",
    "        if not os.path.exists(self.path):\n",
    "            os.makedirs(self.path)\n",
    "        \n",
    "        ############################# Agent & Target #############################\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self.agent_net = DistQNetwork(self.num_actions, num_atoms=num_atoms, scope=\"agent\")\n",
    "        self.target_net = DistQNetwork(self.num_actions, num_atoms=num_atoms, scope=\"target\")\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        all_vars = tf.trainable_variables()\n",
    "        num_vars = len(all_vars) // 2\n",
    "        self.agent_vars = all_vars[:num_vars]\n",
    "        self.target_vars = all_vars[num_vars:]\n",
    "        \n",
    "    def set_parameters(self, \n",
    "                       replay_memory_size=50000,\n",
    "                       replay_start_size=10000,\n",
    "                       init_eps=1,\n",
    "                       final_eps=0.1,\n",
    "                       annealing_steps=100000,\n",
    "                       discount_factor=0.99,\n",
    "                       max_episode_length=2000):\n",
    "        \n",
    "        # create experience replay and fill it with random policy samples\n",
    "        self.rep_buffer = ReplayMemory(replay_memory_size)\n",
    "        frame_count = 0\n",
    "        while (frame_count < replay_start_size):\n",
    "            s = self.train_env.reset()\n",
    "            for time_step in range(max_episode_length):\n",
    "                a = np.random.randint(self.num_actions)\n",
    "                s_, r, end = self.train_env.step(a)\n",
    "                self.rep_buffer.push(s, a, np.sign(r), s_, end)\n",
    "                s = s_\n",
    "                frame_count += 1\n",
    "                if end:\n",
    "                    break\n",
    "                        \n",
    "        self.eps = init_eps\n",
    "        self.final_eps = final_eps\n",
    "        self.eps_drop = (init_eps - final_eps) / annealing_steps\n",
    "        self.gamma = discount_factor\n",
    "        self.max_ep_length = max_episode_length\n",
    "        \n",
    "    def train(self,\n",
    "              gpu_id=0,\n",
    "              batch_size=32,\n",
    "              agent_update_freq=4,\n",
    "              target_update_freq=5000,\n",
    "              tau=1,\n",
    "              max_num_episodes=100000,\n",
    "              max_num_epochs=50000,\n",
    "              performance_print_freq=100,\n",
    "              save_freq=10000):\n",
    "        \n",
    "        target_ops = self.update_target_graph(tau)\n",
    "        os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(self.init)\n",
    "            \n",
    "            train_rewards = []\n",
    "            frame_count = 0\n",
    "            episode_count = 0\n",
    "            num_epochs = 0\n",
    "            \n",
    "            while num_epochs < max_num_epochs:\n",
    "                \n",
    "                train_ep_reward = 0\n",
    "                \n",
    "                # reset the environment / start new game\n",
    "                s = self.train_env.reset()\n",
    "                for time_step in range(self.max_ep_length):\n",
    "                    \n",
    "                    # choose action e-greedily\n",
    "                    if np.random.rand(1) < self.eps:\n",
    "                        a = np.random.randint(self.num_actions)\n",
    "                    else:\n",
    "                        a = self.agent_net.get_q_argmax(sess, [s])\n",
    "                        \n",
    "                    # make step in the environment    \n",
    "                    s_, r, end = self.train_env.step(a)\n",
    "                    \n",
    "                    # save transition into experience replay\n",
    "                    self.rep_buffer.push(s, a, np.sign(r), s_, end)\n",
    "                    \n",
    "                    # update current state and statistics\n",
    "                    s = s_\n",
    "                    frame_count += 1\n",
    "                    train_ep_reward += r\n",
    "                    \n",
    "                    # reduce epsilon according to schedule\n",
    "                    if self.eps > self.final_eps:\n",
    "                        self.eps -= self.eps_drop\n",
    "                    \n",
    "                    # update network weights\n",
    "                    if frame_count % agent_update_freq == 0:\n",
    "                        \n",
    "                        batch = self.rep_buffer.get_batch(batch_size)\n",
    "                        \n",
    "                        # estimate right hand side of the Bellman equation\n",
    "                        max_actions = self.agent_net.get_q_argmax(sess, batch.s_)\n",
    "                        target_m = self.target_net.cat_proj(sess, batch.r, batch.s_, \n",
    "                                                            max_actions, batch.end)\n",
    "                        \n",
    "                        # update agent network\n",
    "                        self.agent_net.update(sess, batch.s, batch.a, target_m)\n",
    "                        \n",
    "                        # update target network\n",
    "                        if tau == 1:\n",
    "                            if frame_count % target_update_freq == 0:\n",
    "                                self.update_target_weights(sess, target_ops)\n",
    "                        else: self.update_target_weights(sess, target_ops)\n",
    "                    \n",
    "                    # make checkpoints of network weights and save learning curve\n",
    "                    if frame_count % save_freq == 1:\n",
    "                        num_epochs += 1\n",
    "                        try:\n",
    "                            self.saver.save(sess, self.path+\"/model\", global_step=num_epochs)\n",
    "                            np.savez(self.path+\"/learning_curve.npz\", r=train_rewards)\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    # if game is over, reset the environment\n",
    "                    if end: \n",
    "                        break\n",
    "                         \n",
    "                episode_count += 1\n",
    "                train_rewards.append(train_ep_reward)\n",
    "                \n",
    "                # print performance once in a while\n",
    "                if episode_count % performance_print_freq == 0:\n",
    "                    avg_reward = np.mean(train_rewards[-performance_print_freq:])\n",
    "                    max_reward = np.max(train_rewards[-performance_print_freq:])\n",
    "                    print(\"Train info:\", frame_count, avg_reward, max_reward)  \n",
    "\n",
    "    def update_target_graph(self, tau):\n",
    "        op_holder = []\n",
    "        for agnt, trgt in zip(self.agent_vars, self.target_vars):\n",
    "            op = trgt.assign(agnt.value()*tau + (1 - tau)*trgt.value())\n",
    "            op_holder.append(op)\n",
    "        return op_holder\n",
    "\n",
    "    def update_target_weights(self, sess, op_holder):\n",
    "        for op in op_holder:\n",
    "            sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = SnakeAgent(model_name=\"baseline_dist3\", num_atoms=51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.set_parameters(max_episode_length=20000, replay_memory_size=50000, replay_start_size=10000, final_eps=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train info: 913 -0.89 0\n",
      "Train info: 1808 -0.87 1\n",
      "Train info: 2630 -0.87 0\n",
      "Train info: 3492 -0.94 0\n",
      "Train info: 4330 -0.91 0\n",
      "Train info: 5128 -0.87 1\n",
      "Train info: 5892 -0.87 1\n",
      "Train info: 6690 -0.9 0\n",
      "Train info: 7504 -0.89 2\n",
      "Train info: 8486 -0.85 1\n",
      "Train info: 9400 -0.87 2\n",
      "Train info: 10297 -0.91 0\n",
      "Train info: 11070 -0.91 0\n",
      "Train info: 11846 -0.92 1\n",
      "Train info: 12777 -0.89 0\n",
      "Train info: 13615 -0.83 1\n",
      "Train info: 14453 -0.92 1\n",
      "Train info: 15331 -0.84 1\n",
      "Train info: 16363 -0.9 0\n",
      "Train info: 17136 -0.9 0\n",
      "Train info: 17931 -0.92 1\n",
      "Train info: 18750 -0.88 0\n",
      "Train info: 19615 -0.86 1\n",
      "Train info: 20430 -0.93 0\n",
      "Train info: 21356 -0.91 0\n",
      "Train info: 22174 -0.93 0\n",
      "Train info: 23066 -0.88 0\n",
      "Train info: 24072 -0.9 2\n",
      "Train info: 24970 -0.94 0\n",
      "Train info: 25813 -0.83 1\n",
      "Train info: 26756 -0.84 0\n",
      "Train info: 27767 -0.9 1\n",
      "Train info: 28826 -0.88 1\n",
      "Train info: 29827 -0.82 1\n",
      "Train info: 30934 -0.9 1\n",
      "Train info: 32073 -0.88 1\n",
      "Train info: 33114 -0.88 0\n",
      "Train info: 34157 -0.93 1\n",
      "Train info: 35070 -0.91 0\n",
      "Train info: 36210 -0.88 0\n",
      "Train info: 37614 -0.81 1\n",
      "Train info: 38948 -0.9 0\n",
      "Train info: 40147 -0.84 2\n",
      "Train info: 41521 -0.88 0\n",
      "Train info: 42779 -0.88 1\n",
      "Train info: 44374 -0.85 0\n",
      "Train info: 45911 -0.87 1\n",
      "Train info: 47653 -0.84 1\n",
      "Train info: 49324 -0.83 0\n",
      "Train info: 50836 -0.85 0\n",
      "Train info: 52868 -0.82 1\n",
      "Train info: 54939 -0.8 1\n",
      "Train info: 57295 -0.73 2\n",
      "Train info: 59920 -0.8 1\n",
      "Train info: 62406 -0.83 2\n",
      "Train info: 65759 -0.78 1\n",
      "Train info: 69480 -0.66 2\n",
      "Train info: 73203 -0.69 1\n",
      "Train info: 77890 -0.7 2\n",
      "Train info: 85010 -0.53 1\n",
      "Train info: 93548 -0.63 2\n",
      "Train info: 115666 -0.36 2\n",
      "Train info: 140777 -0.44 2\n",
      "Train info: 156789 -0.51 1\n",
      "Train info: 171479 -0.63 1\n",
      "Train info: 184318 -0.53 1\n",
      "Train info: 195686 -0.41 4\n",
      "Train info: 206490 -0.3 3\n",
      "Train info: 216716 -0.34 3\n",
      "Train info: 231179 -0.41 3\n",
      "Train info: 244432 -0.2 3\n",
      "Train info: 254290 -0.32 3\n",
      "Train info: 262006 -0.25 4\n",
      "Train info: 271428 -0.27 5\n",
      "Train info: 281693 -0.29 2\n",
      "Train info: 290346 -0.22 3\n",
      "Train info: 298967 -0.04 4\n",
      "Train info: 308200 -0.22 3\n",
      "Train info: 315757 -0.04 4\n",
      "Train info: 325798 -0.08 4\n",
      "Train info: 334017 -0.15 5\n",
      "Train info: 341140 -0.1 3\n",
      "Train info: 350732 -0.03 5\n",
      "Train info: 360474 -0.04 4\n",
      "Train info: 367323 -0.19 3\n",
      "Train info: 375452 -0.06 3\n",
      "Train info: 383058 -0.04 4\n",
      "Train info: 392457 0.1 4\n",
      "Train info: 399144 -0.18 4\n",
      "Train info: 406327 -0.05 4\n",
      "Train info: 413600 0.17 6\n",
      "Train info: 423154 0.16 4\n",
      "Train info: 429762 0.12 6\n",
      "Train info: 438479 0.42 5\n",
      "Train info: 446116 0.09 4\n",
      "Train info: 455513 0.5 4\n",
      "Train info: 463616 0.47 6\n",
      "Train info: 470601 0.55 5\n",
      "Train info: 479166 0.48 4\n",
      "Train info: 486846 0.55 7\n",
      "Train info: 494801 0.61 7\n",
      "Train info: 502116 0.49 7\n",
      "Train info: 508625 0.41 5\n",
      "Train info: 516726 0.33 6\n",
      "Train info: 524753 0.61 6\n",
      "Train info: 531792 0.83 6\n",
      "Train info: 541202 0.89 5\n",
      "Train info: 547378 0.68 7\n",
      "Train info: 554208 0.66 5\n",
      "Train info: 561544 0.86 7\n",
      "Train info: 568298 0.7 5\n",
      "Train info: 574644 0.7 4\n",
      "Train info: 580431 0.94 7\n",
      "Train info: 586961 0.78 6\n",
      "Train info: 594417 1.57 8\n",
      "Train info: 602547 1.26 6\n",
      "Train info: 610636 1.35 6\n",
      "Train info: 618038 1.14 6\n",
      "Train info: 624613 1.09 7\n",
      "Train info: 631374 1.35 8\n",
      "Train info: 637264 1.56 9\n",
      "Train info: 642830 1.54 7\n",
      "Train info: 648546 1.58 8\n",
      "Train info: 654513 1.45 8\n",
      "Train info: 659305 1.53 8\n",
      "Train info: 665588 1.49 8\n",
      "Train info: 671685 1.51 8\n",
      "Train info: 678417 1.68 9\n",
      "Train info: 683301 1.38 8\n",
      "Train info: 689058 1.37 6\n",
      "Train info: 694576 1.5 9\n",
      "Train info: 700347 1.71 8\n",
      "Train info: 705695 2.26 9\n",
      "Train info: 710759 1.62 8\n",
      "Train info: 716643 1.69 7\n",
      "Train info: 722422 1.63 8\n",
      "Train info: 728609 1.84 8\n",
      "Train info: 734676 1.8 8\n",
      "Train info: 740410 1.91 8\n",
      "Train info: 746692 2.32 10\n",
      "Train info: 753328 2.23 10\n",
      "Train info: 760197 2.01 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.5/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'reshape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7905ecf3ae85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-cc18b6d07967>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, gpu_id, batch_size, agent_update_freq, target_update_freq, tau, max_num_episodes, max_num_epochs, performance_print_freq, save_freq)\u001b[0m\n\u001b[1;32m    123\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mframe_count\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0magent_update_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrep_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0;31m# estimate right hand side of the Bellman equation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/snake_research/methods.py\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.5/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    255\u001b[0m            [5, 6]])\n\u001b[1;32m    256\u001b[0m     \"\"\"\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.5/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# a downstream library like 'pandas'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.5/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "aa.train(gpu_id=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_atoms = 101\n",
    "tf.reset_default_graph()\n",
    "agent_net = DistQNetwork(4, num_atoms=num_atoms, scope=\"agent\")\n",
    "saver = tf.train.Saver()\n",
    "env = Snake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(3)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "viz = AgentViz(sess, 'snake_models/baseline_dist3/model-30', agent_net, max_frames=200, grid_size=(2, 5), figsize=(20, 8))\n",
    "anim = matplotlib.animation.FuncAnimation(viz.fig, viz, frames=viz.max_frames)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
