{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathemage/anaconda3/envs/tf/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/mathemage/anaconda3/envs/tf/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from PIL import Image\n",
    "from collections import deque, namedtuple\n",
    "from IPython.display import HTML\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from environments import Snake\n",
    "from visual_utils import AgentViz\n",
    "from methods import ReflexDistQNetwork,  ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1cf30630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = Snake()\n",
    "img = s.reset()\n",
    "s.plot_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SnakeAgent:\n",
    "    \n",
    "    def __init__(self, num_atoms=21, reflex=3, model_name=\"baseline_agent\"):\n",
    "        \n",
    "        \"\"\"Class for training and evaluating DQN agent on Atari games\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        game_id: str\n",
    "            game identifier in gym environment, e.g. \"Pong\"\n",
    "        num_actions: int\n",
    "            number of actions the agent can take\n",
    "        model_name: str\n",
    "            name of the model\n",
    "        \"\"\"\n",
    "        \n",
    "        ############################ Game environment ############################\n",
    "        \n",
    "        self.train_env = Snake()\n",
    "        self.num_actions = 4\n",
    "        self.reflex = reflex\n",
    "            \n",
    "        self.path = \"snake_models\" + \"/\" + model_name\n",
    "        if not os.path.exists(self.path):\n",
    "            os.makedirs(self.path)\n",
    "        \n",
    "        ############################# Agent & Target #############################\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self.agent_net = ReflexDistQNetwork(self.num_actions, num_atoms=num_atoms, reflex=reflex, scope=\"agent\")\n",
    "        self.target_net = ReflexDistQNetwork(self.num_actions, num_atoms=num_atoms, reflex=reflex, scope=\"target\")\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        all_vars = tf.trainable_variables()\n",
    "        num_vars = len(all_vars) // 2\n",
    "        self.agent_vars = all_vars[:num_vars]\n",
    "        self.target_vars = all_vars[num_vars:]\n",
    "        \n",
    "    def set_parameters(self, \n",
    "                       replay_memory_size=50000,\n",
    "                       replay_start_size=10000,\n",
    "                       init_eps=1,\n",
    "                       final_eps=0.1,\n",
    "                       annealing_steps=100000,\n",
    "                       discount_factor=0.99,\n",
    "                       max_episode_length=2000):\n",
    "        \n",
    "        # create experience replay and fill it with random policy samples\n",
    "        self.rep_buffer = ReplayMemory(replay_memory_size)\n",
    "        frame_count = 0\n",
    "        while (frame_count < replay_start_size):\n",
    "            s = self.train_env.reset()\n",
    "            for time_step in range(max_episode_length):\n",
    "                a = np.random.randint(self.num_actions)\n",
    "                s_, r, end = self.train_env.step(a)\n",
    "                self.rep_buffer.push(s, a, np.sign(r), s_, end)\n",
    "                s = s_\n",
    "                frame_count += 1\n",
    "                if end:\n",
    "                    break\n",
    "                        \n",
    "        self.eps = init_eps\n",
    "        self.final_eps = final_eps\n",
    "        self.eps_drop = (init_eps - final_eps) / annealing_steps\n",
    "        self.gamma = discount_factor\n",
    "        self.max_ep_length = max_episode_length\n",
    "        \n",
    "    def train(self,\n",
    "              gpu_id=0,\n",
    "              batch_size=32,\n",
    "              agent_update_freq=4,\n",
    "              target_update_freq=5000,\n",
    "              tau=1,\n",
    "              max_num_episodes=100000,\n",
    "              max_num_epochs=50000,\n",
    "              performance_print_freq=100,\n",
    "              save_freq=10000):\n",
    "        \n",
    "        target_ops = self.update_target_graph(tau)\n",
    "        os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(self.init)\n",
    "            \n",
    "            train_rewards = []\n",
    "            frame_count = 0\n",
    "            episode_count = 0\n",
    "            num_epochs = 0\n",
    "            \n",
    "            while num_epochs < max_num_epochs:\n",
    "                \n",
    "                train_ep_reward = 0\n",
    "                \n",
    "                # reset the environment / start new game\n",
    "                s = self.train_env.reset()\n",
    "                for time_step in range(self.max_ep_length):\n",
    "                    \n",
    "                    a = self.agent_net.get_q_argmax(sess, [s])\n",
    "                    s_, r, end = self.train_env.step(a)\n",
    "                                    \n",
    "                    # save transition into experience replay\n",
    "                    self.rep_buffer.push(s, a, r, s_, end)\n",
    "                    \n",
    "                    # update current state and statistics\n",
    "                    s = s_\n",
    "                    frame_count += 1\n",
    "                    train_ep_reward += r\n",
    "                    \n",
    "                    # reduce epsilon according to schedule\n",
    "                    if self.eps > self.final_eps:\n",
    "                        self.eps -= self.eps_drop\n",
    "                    \n",
    "                    # update network weights\n",
    "                    if frame_count % agent_update_freq == 0:\n",
    "                        \n",
    "                        batch = self.rep_buffer.get_batch(batch_size)\n",
    "                        \n",
    "                        # estimate right hand side of the Bellman equation\n",
    "                        max_actions = self.agent_net.get_q_argmax(sess, batch.s_)\n",
    "                        target_m = self.target_net.cat_proj(sess, batch.r, batch.s_, \n",
    "                                                            max_actions, batch.end)\n",
    "                        \n",
    "                        # update agent network\n",
    "                        self.agent_net.update(sess, batch.s, batch.a, target_m)\n",
    "                        \n",
    "                        # update target network\n",
    "                        if tau == 1:\n",
    "                            if frame_count % target_update_freq == 0:\n",
    "                                self.update_target_weights(sess, target_ops)\n",
    "                        else: self.update_target_weights(sess, target_ops)\n",
    "                    \n",
    "                    # make checkpoints of network weights and save learning curve\n",
    "                    if frame_count % save_freq == 1:\n",
    "                        num_epochs += 1\n",
    "                        try:\n",
    "                            self.saver.save(sess, self.path+\"/model\", global_step=num_epochs)\n",
    "                            np.savez(self.path+\"/learning_curve.npz\", r=train_rewards)\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    # if game is over, reset the environment\n",
    "                    if end: \n",
    "                        break\n",
    "                         \n",
    "                episode_count += 1\n",
    "                train_rewards.append(train_ep_reward)\n",
    "                \n",
    "                # print performance once in a while\n",
    "                if episode_count % performance_print_freq == 0:\n",
    "                    avg_reward = np.mean(train_rewards[-performance_print_freq:])\n",
    "                    max_reward = np.max(train_rewards[-performance_print_freq:])\n",
    "                    print(\"Train info:\", frame_count, avg_reward, max_reward)  \n",
    "\n",
    "    def update_target_graph(self, tau):\n",
    "        op_holder = []\n",
    "        for agnt, trgt in zip(self.agent_vars, self.target_vars):\n",
    "            op = trgt.assign(agnt.value()*tau + (1 - tau)*trgt.value())\n",
    "            op_holder.append(op)\n",
    "        return op_holder\n",
    "\n",
    "    def update_target_weights(self, sess, op_holder):\n",
    "        for op in op_holder:\n",
    "            sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa = SnakeAgent(model_name=\"baseline_reflqn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa.set_parameters(max_episode_length=50000, replay_memory_size=50000, replay_start_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train info: 437 -0.96 0\n",
      "Train info: 929 -0.94 0\n",
      "Train info: 1474 -0.92 1\n",
      "Train info: 2067 -0.97 0\n",
      "Train info: 2635 -0.89 0\n",
      "Train info: 3194 -0.97 0\n",
      "Train info: 3734 -0.92 0\n",
      "Train info: 4335 -0.95 0\n",
      "Train info: 4859 -0.97 0\n",
      "Train info: 5376 -0.95 0\n",
      "Train info: 5937 -0.95 0\n",
      "Train info: 6525 -0.95 0\n",
      "Train info: 7113 -0.95 0\n",
      "Train info: 7621 -0.93 0\n",
      "Train info: 8096 -0.96 0\n",
      "Train info: 8628 -0.93 0\n",
      "Train info: 9154 -0.94 1\n",
      "Train info: 9712 -0.92 0\n",
      "Train info: 10218 -0.94 0\n",
      "Train info: 10755 -0.92 0\n",
      "Train info: 11352 -0.93 0\n",
      "Train info: 11931 -0.86 1\n",
      "Train info: 12478 -0.96 0\n",
      "Train info: 13079 -0.91 1\n",
      "Train info: 13594 -0.93 0\n",
      "Train info: 14097 -0.91 1\n",
      "Train info: 14572 -0.88 1\n",
      "Train info: 15118 -0.96 0\n",
      "Train info: 15647 -0.92 0\n",
      "Train info: 16171 -0.92 0\n",
      "Train info: 16635 -0.9 0\n",
      "Train info: 17163 -0.89 0\n",
      "Train info: 17766 -0.88 0\n",
      "Train info: 18285 -0.89 1\n",
      "Train info: 18815 -0.89 0\n",
      "Train info: 19363 -0.94 2\n",
      "Train info: 19888 -0.9 1\n",
      "Train info: 20417 -0.92 2\n",
      "Train info: 20986 -0.96 1\n",
      "Train info: 21493 -0.92 0\n",
      "Train info: 22079 -0.94 0\n",
      "Train info: 22621 -0.95 0\n",
      "Train info: 23132 -0.92 0\n",
      "Train info: 23679 -0.94 0\n",
      "Train info: 24331 -0.96 0\n",
      "Train info: 24916 -0.94 0\n",
      "Train info: 25404 -0.93 0\n",
      "Train info: 25942 -0.94 0\n",
      "Train info: 26504 -0.94 0\n",
      "Train info: 27102 -0.92 1\n",
      "Train info: 27680 -0.92 0\n",
      "Train info: 28307 -0.88 0\n",
      "Train info: 28832 -0.92 1\n",
      "Train info: 29354 -0.93 0\n",
      "Train info: 29992 -0.87 1\n",
      "Train info: 30557 -0.98 0\n",
      "Train info: 31094 -0.96 0\n",
      "Train info: 31628 -0.94 0\n",
      "Train info: 32162 -0.87 1\n",
      "Train info: 32729 -0.84 1\n",
      "Train info: 33202 -0.89 1\n",
      "Train info: 33696 -0.94 1\n",
      "Train info: 34209 -0.93 0\n",
      "Train info: 34731 -0.9 1\n",
      "Train info: 35299 -0.91 1\n",
      "Train info: 35843 -0.92 0\n",
      "Train info: 36362 -0.9 1\n",
      "Train info: 36919 -0.82 1\n",
      "Train info: 37377 -0.9 0\n",
      "Train info: 37879 -0.8 1\n",
      "Train info: 38404 -0.86 1\n",
      "Train info: 38992 -0.86 1\n",
      "Train info: 39515 -0.9 0\n",
      "Train info: 40002 -0.93 0\n",
      "Train info: 40464 -0.88 1\n",
      "Train info: 41014 -0.92 1\n",
      "Train info: 41547 -0.9 0\n",
      "Train info: 42058 -0.86 0\n",
      "Train info: 42565 -0.8 1\n",
      "Train info: 43077 -0.87 1\n",
      "Train info: 43565 -0.84 2\n",
      "Train info: 44003 -0.96 0\n",
      "Train info: 44425 -0.85 1\n",
      "Train info: 44976 -0.86 1\n",
      "Train info: 45453 -0.86 1\n",
      "Train info: 45897 -0.91 1\n",
      "Train info: 46470 -0.76 1\n",
      "Train info: 46987 -0.87 1\n",
      "Train info: 47543 -0.85 1\n",
      "Train info: 48070 -0.86 1\n",
      "Train info: 48620 -0.87 0\n",
      "Train info: 49109 -0.8 1\n",
      "Train info: 49614 -0.9 1\n",
      "Train info: 50075 -0.86 1\n",
      "Train info: 50523 -0.89 1\n",
      "Train info: 50953 -0.92 0\n",
      "Train info: 51423 -0.88 1\n",
      "Train info: 51852 -0.85 1\n",
      "Train info: 52340 -0.83 1\n",
      "Train info: 52864 -0.83 1\n",
      "Train info: 53395 -0.81 1\n",
      "Train info: 53865 -0.86 1\n",
      "Train info: 54324 -0.84 2\n",
      "Train info: 54833 -0.82 1\n",
      "Train info: 55259 -0.85 0\n",
      "Train info: 55753 -0.91 1\n",
      "Train info: 56247 -0.84 0\n",
      "Train info: 56702 -0.84 0\n",
      "Train info: 57178 -0.82 2\n",
      "Train info: 57672 -0.79 1\n",
      "Train info: 58148 -0.82 2\n",
      "Train info: 58664 -0.79 3\n",
      "Train info: 59174 -0.87 1\n",
      "Train info: 59735 -0.73 1\n",
      "Train info: 60180 -0.87 0\n",
      "Train info: 60597 -0.86 0\n",
      "Train info: 60983 -0.8 1\n",
      "Train info: 61427 -0.73 2\n",
      "Train info: 61920 -0.85 1\n",
      "Train info: 62352 -0.81 1\n",
      "Train info: 62842 -0.82 1\n",
      "Train info: 63273 -0.81 1\n",
      "Train info: 63733 -0.85 0\n",
      "Train info: 64161 -0.86 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d99875174f7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-0eb7d0ee3ed7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, gpu_id, batch_size, agent_update_freq, target_update_freq, tau, max_num_episodes, max_num_epochs, performance_print_freq, save_freq)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0;31m# update agent network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                         \u001b[0;31m# update target network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/snake_research/methods.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, sess, states, actions, targets)\u001b[0m\n\u001b[1;32m    644\u001b[0m                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_actions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m                      self.targets:targets}\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcat_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "aa.train(gpu_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tf)",
   "language": "python",
   "name": "conda_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
