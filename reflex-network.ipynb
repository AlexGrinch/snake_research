{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathemage/anaconda3/envs/tf/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/mathemage/anaconda3/envs/tf/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from PIL import Image\n",
    "from collections import deque, namedtuple\n",
    "from IPython.display import HTML\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from environments import Snake\n",
    "from visual_utils import AgentViz\n",
    "from methods import ReflexDistQNetwork,  ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c28e3e0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = Snake()\n",
    "img = s.reset()\n",
    "s.plot_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SnakeAgent:\n",
    "    \n",
    "    def __init__(self, num_atoms=21, reflex=3, model_name=\"baseline_agent\"):\n",
    "        \n",
    "        \"\"\"Class for training and evaluating DQN agent on Atari games\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        game_id: str\n",
    "            game identifier in gym environment, e.g. \"Pong\"\n",
    "        num_actions: int\n",
    "            number of actions the agent can take\n",
    "        model_name: str\n",
    "            name of the model\n",
    "        \"\"\"\n",
    "        \n",
    "        ############################ Game environment ############################\n",
    "        \n",
    "        self.train_env = Snake()\n",
    "        self.num_actions = 4\n",
    "        self.reflex = reflex\n",
    "            \n",
    "        self.path = \"snake_models\" + \"/\" + model_name\n",
    "        if not os.path.exists(self.path):\n",
    "            os.makedirs(self.path)\n",
    "        \n",
    "        ############################# Agent & Target #############################\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self.agent_net = ReflexDistQNetwork(self.num_actions, num_atoms=num_atoms, reflex=reflex, scope=\"agent\")\n",
    "        self.target_net = ReflexDistQNetwork(self.num_actions, num_atoms=num_atoms, reflex=reflex, scope=\"target\")\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        all_vars = tf.trainable_variables()\n",
    "        num_vars = len(all_vars) // 2\n",
    "        self.agent_vars = all_vars[:num_vars]\n",
    "        self.target_vars = all_vars[num_vars:]\n",
    "        \n",
    "    def set_parameters(self, \n",
    "                       replay_memory_size=50000,\n",
    "                       replay_start_size=10000,\n",
    "                       init_eps=1,\n",
    "                       final_eps=0.1,\n",
    "                       annealing_steps=100000,\n",
    "                       discount_factor=0.99,\n",
    "                       max_episode_length=2000):\n",
    "        \n",
    "        # create experience replay and fill it with random policy samples\n",
    "        self.rep_buffer = ReplayMemory(replay_memory_size)\n",
    "        frame_count = 0\n",
    "        while (frame_count < replay_start_size):\n",
    "            s = self.train_env.reset()\n",
    "            for time_step in range(max_episode_length):\n",
    "                a = np.random.randint(self.num_actions)\n",
    "                s_, r, end = self.train_env.step(a)\n",
    "                self.rep_buffer.push(s, a, np.sign(r), s_, end)\n",
    "                s = s_\n",
    "                frame_count += 1\n",
    "                if end:\n",
    "                    break\n",
    "                        \n",
    "        self.eps = init_eps\n",
    "        self.final_eps = final_eps\n",
    "        self.eps_drop = (init_eps - final_eps) / annealing_steps\n",
    "        self.gamma = discount_factor\n",
    "        self.max_ep_length = max_episode_length\n",
    "        \n",
    "    def train(self,\n",
    "              gpu_id=0,\n",
    "              batch_size=32,\n",
    "              agent_update_freq=4,\n",
    "              target_update_freq=5000,\n",
    "              tau=1,\n",
    "              max_num_episodes=100000,\n",
    "              max_num_epochs=50000,\n",
    "              performance_print_freq=100,\n",
    "              save_freq=10000):\n",
    "        \n",
    "        target_ops = self.update_target_graph(tau)\n",
    "        os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(self.init)\n",
    "            \n",
    "            train_rewards = []\n",
    "            frame_count = 0\n",
    "            episode_count = 0\n",
    "            num_epochs = 0\n",
    "            \n",
    "            while num_epochs < max_num_epochs:\n",
    "                \n",
    "                train_ep_reward = 0\n",
    "                \n",
    "                # reset the environment / start new game\n",
    "                s = self.train_env.reset()\n",
    "                for time_step in range(self.max_ep_length):\n",
    "                    \n",
    "                    # get sequence of actions using lstm\n",
    "                    if np.random.rand() < self.eps:\n",
    "                        a = np.random.randint(0, self.num_actions, size=(self.reflex, ))\n",
    "                    a = self.agent_net.get_action_seq(sess, [s])[0]\n",
    "                    total_r = 0\n",
    "                    for a_idx, a_ in enumerate(a):\n",
    "                        s_, r, end = self.train_env.step(a_)\n",
    "                        total_r += (self.gamma**a_idx) * np.sign(r)\n",
    "                        if end: break\n",
    "                    \n",
    "                    r = total_r\n",
    "                                    \n",
    "                    # save transition into experience replay\n",
    "                    self.rep_buffer.push(s, a[0], r, s_, end)\n",
    "                    \n",
    "                    # update current state and statistics\n",
    "                    s = s_\n",
    "                    frame_count += 1\n",
    "                    train_ep_reward += r\n",
    "                    \n",
    "                    # reduce epsilon according to schedule\n",
    "                    if self.eps > self.final_eps:\n",
    "                        self.eps -= self.eps_drop\n",
    "                    \n",
    "                    # update network weights\n",
    "                    if frame_count % agent_update_freq == 0:\n",
    "                        \n",
    "                        batch = self.rep_buffer.get_batch(batch_size)\n",
    "                        \n",
    "                        # estimate right hand side of the Bellman equation\n",
    "                        max_actions = self.agent_net.get_q_argmax(sess, batch.s_)\n",
    "                        target_m = self.target_net.cat_proj(sess, batch.r, batch.s_, \n",
    "                                                            max_actions, batch.end)\n",
    "                        \n",
    "                        # update agent network\n",
    "                        self.agent_net.update(sess, batch.s, batch.a, target_m)\n",
    "                        \n",
    "                        # update target network\n",
    "                        if tau == 1:\n",
    "                            if frame_count % target_update_freq == 0:\n",
    "                                self.update_target_weights(sess, target_ops)\n",
    "                        else: self.update_target_weights(sess, target_ops)\n",
    "                    \n",
    "                    # make checkpoints of network weights and save learning curve\n",
    "                    if frame_count % save_freq == 1:\n",
    "                        num_epochs += 1\n",
    "                        try:\n",
    "                            self.saver.save(sess, self.path+\"/model\", global_step=num_epochs)\n",
    "                            np.savez(self.path+\"/learning_curve.npz\", r=train_rewards)\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    # if game is over, reset the environment\n",
    "                    if end: \n",
    "                        break\n",
    "                         \n",
    "                episode_count += 1\n",
    "                train_rewards.append(train_ep_reward)\n",
    "                \n",
    "                # print performance once in a while\n",
    "                if episode_count % performance_print_freq == 0:\n",
    "                    avg_reward = np.mean(train_rewards[-performance_print_freq:])\n",
    "                    max_reward = np.max(train_rewards[-performance_print_freq:])\n",
    "                    print(\"Train info:\", frame_count, avg_reward, max_reward)  \n",
    "\n",
    "    def update_target_graph(self, tau):\n",
    "        op_holder = []\n",
    "        for agnt, trgt in zip(self.agent_vars, self.target_vars):\n",
    "            op = trgt.assign(agnt.value()*tau + (1 - tau)*trgt.value())\n",
    "            op_holder.append(op)\n",
    "        return op_holder\n",
    "\n",
    "    def update_target_weights(self, sess, op_holder):\n",
    "        for op in op_holder:\n",
    "            sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = SnakeAgent(model_name=\"baseline_reflqn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa.set_parameters(max_episode_length=50000, replay_memory_size=50000, replay_start_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train info: 174 -0.9803320000000001 0.01990000000000003\n",
      "Train info: 374 -0.972223 0.010000000000000009\n",
      "Train info: 570 -0.9026209999999999 0.01990000000000003\n",
      "Train info: 757 -0.9530189999999998 0.01990000000000003\n",
      "Train info: 929 -0.9414219999999999 0.01990000000000003\n",
      "Train info: 1113 -0.912824 0.01990000000000003\n",
      "Train info: 1288 -0.9408299999999999 0.01990000000000003\n",
      "Train info: 1470 -0.9316299999999998 0.01990000000000003\n",
      "Train info: 1655 -0.9126240000000001 0.010000000000000009\n",
      "Train info: 1844 -0.9225209999999998 0.01990000000000003\n",
      "Train info: 2039 -0.9114260000000001 0.01990000000000003\n",
      "Train info: 2215 -0.9205300000000001 0.01990000000000003\n",
      "Train info: 2406 -0.9226199999999999 0.01990000000000003\n",
      "Train info: 2587 -0.9720210000000001 0.010000000000000009\n",
      "Train info: 2760 -0.9016239999999999 0.99\n",
      "Train info: 2935 -0.9413279999999999 0.01990000000000003\n",
      "Train info: 3106 -0.9402349999999999 1.0099\n",
      "Train info: 3295 -0.960531 0.01990000000000003\n",
      "Train info: 3479 -0.940732 0.01990000000000003\n",
      "Train info: 3661 -0.9417309999999999 1.0\n",
      "Train info: 3836 -0.9326239999999999 0.01990000000000003\n",
      "Train info: 4015 -0.9634170000000001 0.01990000000000003\n",
      "Train info: 4195 -0.9526230000000001 0.01990000000000003\n",
      "Train info: 4385 -0.952921 0.010000000000000009\n",
      "Train info: 4573 -0.970826 0.01990000000000003\n",
      "Train info: 4747 -0.930034 0.01990000000000003\n",
      "Train info: 4928 -0.9105339999999998 0.01990000000000003\n",
      "Train info: 5119 -0.9422210000000001 0.01990000000000003\n",
      "Train info: 5297 -0.9538149999999999 0.01990000000000003\n",
      "Train info: 5469 -0.9520259999999999 0.00990000000000002\n",
      "Train info: 5651 -0.9523259999999999 0.010000000000000009\n",
      "Train info: 5815 -0.8711290000000002 0.9702\n",
      "Train info: 5992 -0.9723209999999999 0.01990000000000003\n",
      "Train info: 6174 -0.925112 0.00990000000000002\n",
      "Train info: 6371 -0.9421259999999998 0.01990000000000003\n",
      "Train info: 6572 -0.9722259999999998 0.00990000000000002\n",
      "Train info: 6780 -0.9416289999999999 0.00990000000000002\n",
      "Train info: 6971 -0.90342 0.00990000000000002\n",
      "Train info: 7161 -0.9320269999999999 0.00990000000000002\n",
      "Train info: 7353 -0.892627 0.01990000000000003\n",
      "Train info: 7539 -0.9432209999999999 0.01990000000000003\n",
      "Train info: 7728 -0.9016239999999999 0.01990000000000003\n",
      "Train info: 7922 -0.9102319999999997 0.01990000000000003\n",
      "Train info: 8103 -0.951528 0.98\n",
      "Train info: 8294 -0.9619239999999999 0.010000000000000009\n",
      "Train info: 8476 -0.930925 0.010000000000000009\n",
      "Train info: 8668 -0.940634 0.010000000000000009\n",
      "Train info: 8865 -0.9416289999999999 0.01990000000000003\n",
      "Train info: 9059 -0.9323229999999999 0.00990000000000002\n",
      "Train info: 9258 -0.9422189999999998 0.010000000000000009\n",
      "Train info: 9440 -0.8618269999999998 0.99\n",
      "Train info: 9633 -0.942026 0.010000000000000009\n",
      "Train info: 9826 -0.8934199999999999 0.010000000000000009\n",
      "Train info: 10020 -0.9727229999999999 0.00990000000000002\n",
      "Train info: 10218 -0.9246110000000002 0.010000000000000009\n",
      "Train info: 10412 -0.9214269999999999 0.01990000000000003\n",
      "Train info: 10621 -0.952028 0.010000000000000009\n",
      "Train info: 10831 -0.952626 0.98\n",
      "Train info: 11022 -0.9617239999999999 0.01990000000000003\n",
      "Train info: 11208 -0.9415289999999997 0.0\n",
      "Train info: 11398 -0.952026 0.01990000000000003\n",
      "Train info: 11609 -0.9419209999999998 0.010000000000000009\n",
      "Train info: 11803 -0.912427 0.01990000000000003\n",
      "Train info: 11997 -0.913219 0.9801\n",
      "Train info: 12213 -0.913522 0.99\n",
      "Train info: 12404 -0.941228 0.010000000000000009\n",
      "Train info: 12600 -0.943119 0.010000000000000009\n",
      "Train info: 12792 -0.9714269999999999 0.010000000000000009\n",
      "Train info: 12979 -0.942221 0.010000000000000009\n",
      "Train info: 13176 -0.9223239999999998 0.010000000000000009\n",
      "Train info: 13362 -0.9314279999999998 0.01990000000000003\n",
      "Train info: 13563 -0.9124269999999999 0.010000000000000009\n",
      "Train info: 13766 -0.8839169999999998 0.01990000000000003\n",
      "Train info: 13962 -0.912717 0.9901\n",
      "Train info: 14170 -0.932027 0.00990000000000002\n",
      "Train info: 14377 -0.941723 0.010000000000000009\n",
      "Train info: 14587 -0.89252 0.01990000000000003\n",
      "Train info: 14791 -0.931426 0.010000000000000009\n",
      "Train info: 14985 -0.962123 0.010000000000000009\n",
      "Train info: 15186 -0.9324219999999999 0.01990000000000003\n",
      "Train info: 15367 -0.931629 0.010000000000000009\n",
      "Train info: 15568 -0.921729 0.01990000000000003\n",
      "Train info: 15774 -0.9619259999999998 0.00990000000000002\n",
      "Train info: 15967 -0.912127 0.01990000000000003\n",
      "Train info: 16184 -0.912919 0.01990000000000003\n",
      "Train info: 16398 -0.9728160000000001 0.9801\n",
      "Train info: 16603 -0.882523 0.01990000000000003\n",
      "Train info: 16802 -0.891926 2.0\n",
      "Train info: 17014 -0.960729 0.010000000000000009\n",
      "Train info: 17208 -0.9214249999999998 0.01990000000000003\n",
      "Train info: 17413 -0.9427179999999998 0.010000000000000009\n",
      "Train info: 17605 -0.9306329999999998 1.9701\n",
      "Train info: 17813 -0.9022219999999999 0.01990000000000003\n",
      "Train info: 18000 -0.94123 0.010000000000000009\n",
      "Train info: 18197 -0.892423 0.01990000000000003\n",
      "Train info: 18397 -0.9123259999999997 0.00990000000000002\n",
      "Train info: 18589 -0.941326 0.01990000000000003\n",
      "Train info: 18787 -0.9317259999999998 0.01990000000000003\n",
      "Train info: 18987 -0.9225220000000001 0.010000000000000009\n",
      "Train info: 19187 -0.931924 0.010000000000000009\n",
      "Train info: 19370 -0.95053 0.01990000000000003\n",
      "Train info: 19561 -0.951624 0.01990000000000003\n",
      "Train info: 19746 -0.9415229999999998 0.01990000000000003\n",
      "Train info: 19942 -0.9324259999999999 0.01990000000000003\n",
      "Train info: 20141 -0.9516269999999999 0.0\n",
      "Train info: 20341 -0.9218209999999999 0.99\n",
      "Train info: 20529 -0.9109309999999999 0.01990000000000003\n",
      "Train info: 20719 -0.9313330000000001 0.98\n",
      "Train info: 20913 -0.931723 0.010000000000000009\n",
      "Train info: 21105 -0.9504290000000001 0.010000000000000009\n",
      "Train info: 21307 -0.9421239999999999 0.01990000000000003\n",
      "Train info: 21499 -0.9324209999999999 0.00990000000000002\n",
      "Train info: 21701 -0.9505319999999999 0.010000000000000009\n",
      "Train info: 21899 -0.913219 0.01990000000000003\n",
      "Train info: 22088 -0.950926 0.01990000000000003\n",
      "Train info: 22281 -0.9413239999999999 0.010000000000000009\n",
      "Train info: 22465 -0.9503309999999999 0.010000000000000009\n",
      "Train info: 22659 -0.901228 0.01990000000000003\n",
      "Train info: 22845 -0.9226199999999999 0.9801\n",
      "Train info: 23042 -0.9418209999999998 0.01990000000000003\n",
      "Train info: 23246 -0.9019299999999998 1.0\n",
      "Train info: 23443 -0.9621259999999999 0.010000000000000009\n",
      "Train info: 23637 -0.912419 0.010000000000000009\n",
      "Train info: 23842 -0.9015229999999999 0.01990000000000003\n",
      "Train info: 24030 -0.9711249999999998 -0.00990000000000002\n",
      "Train info: 24224 -0.962815 -0.00990000000000002\n",
      "Train info: 24416 -0.9210329999999999 0.01990000000000003\n",
      "Train info: 24612 -0.8805299999999999 1.0\n",
      "Train info: 24800 -0.9129169999999999 0.010000000000000009\n",
      "Train info: 24998 -0.9715289999999999 0.00990000000000002\n",
      "Train info: 25186 -0.9305319999999999 0.010000000000000009\n",
      "Train info: 25362 -0.9319259999999998 0.9801\n",
      "Train info: 25552 -0.9417209999999998 0.0\n",
      "Train info: 25732 -0.9493379999999998 0.010000000000000009\n",
      "Train info: 25917 -0.9417269999999999 0.01990000000000003\n",
      "Train info: 26109 -0.9519239999999999 0.01990000000000003\n",
      "Train info: 26300 -0.9414249999999998 0.01990000000000003\n",
      "Train info: 26485 -0.9705320000000001 0.01990000000000003\n",
      "Train info: 26680 -0.961828 0.01990000000000003\n",
      "Train info: 26855 -0.9406319999999999 0.01990000000000003\n",
      "Train info: 27047 -0.881631 0.010000000000000009\n",
      "Train info: 27232 -0.893222 0.01990000000000003\n",
      "Train info: 27420 -0.9416229999999999 0.01990000000000003\n",
      "Train info: 27595 -0.9218249999999999 0.01990000000000003\n",
      "Train info: 27779 -0.9010219999999998 0.010000000000000009\n",
      "Train info: 27962 -0.932025 0.010000000000000009\n",
      "Train info: 28145 -0.921129 0.00990000000000002\n",
      "Train info: 28347 -0.9330179999999999 0.01990000000000003\n",
      "Train info: 28519 -0.911728 0.01990000000000003\n",
      "Train info: 28723 -0.973017 0.010000000000000009\n",
      "Train info: 28917 -0.8921229999999999 0.01990000000000003\n",
      "Train info: 29102 -0.9237159999999999 0.01990000000000003\n",
      "Train info: 29298 -0.941526 0.01990000000000003\n",
      "Train info: 29472 -0.9409319999999997 0.01990000000000003\n",
      "Train info: 29655 -0.951821 0.01990000000000003\n",
      "Train info: 29840 -0.9233199999999998 0.01990000000000003\n"
     ]
    }
   ],
   "source": [
    "aa.train(gpu_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tf)",
   "language": "python",
   "name": "conda_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
